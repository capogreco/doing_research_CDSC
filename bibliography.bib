@book{alexandrakiMusiCoLabModularArchitecture2022,
  title = {{{MusiCoLab}}: {{Towards}} a {{Modular Architecture}} for {{Collaborative Music Learning}}},
  shorttitle = {{{MusiCoLab}}},
  author = {Alexandraki, Chrisoula and Akoumianakis, Demosthenes and Kalochristianakis, Michael and Zervas, Panagiotis and Kaliakatsos-Papakostas, Maximos and Cambouropoulos, Emilios},
  date = {2022-07-14},
  doi = {10.5281/zenodo.6770559},
  abstract = {This paper presents current developments taking place in the context of the MusiCoLab project. MusiCoLab aims at delivering a comprehensive and efficient web platform for music learning and teaching, by building on prior research experience of project partners, as well as by investigating the integration of state-of-the-art tools in intelligent music composition, performance, and discovery within an educational context. Compared to existing relevant initiatives, MusiCoLab offers a suite of innovative tools that may be used to enhance collaboration and engagement in networked/virtual settings. These tools are sought both in the context of asynchronous student/teacher interactions (i.e., course preparation and scheduling, student assignments and self-practice) as well as synchronously, i.e., serving as groupware to facilitate live music lessons by manipulating intelligent collaborative digital artifacts.}
}

@article{barbosaDisplacedSoundscapesSurvey2003,
  title = {Displaced {{Soundscapes}}: {{A Survey}} of {{Network Systems}} for {{Music}} and {{Sonic Art Creation}}},
  shorttitle = {Displaced {{Soundscapes}}},
  author = {Barbosa, Alvaro},
  date = {2003},
  journaltitle = {Leonardo music journal},
  volume = {13},
  pages = {53--59},
  publisher = {MIT Press},
  location = {238 Main St., Suite 500, Cambridge, MA 02142-1046, USA},
  issn = {0961-1215},
  doi = {10.1162/096112104322750791},
  abstract = {The introduction of various collaborative tools, made possible by the expansion of computer network systems and communications technology, has led to new methods of musical composition and improvisation. The author describes a number of recent music and sound art projects involving the use of network systems that enable geographically displaced creators to collaboratively generate shared soundscapes. Various system designs, ideas and concepts associated with this interaction paradigm are presented and classified by the author.},
  langid = {english},
  keywords = {Composers,Composition (Music),Computer networks,Music,Musical instruments}
}

@book{barrettExperimentingHumanArt2023,
  title = {Experimenting the {{Human}}: {{Art}}, {{Music}}, and the {{Contemporary Posthuman}}},
  shorttitle = {Experimenting the {{Human}}},
  author = {Barrett, G. Douglas},
  date = {2023-01},
  publisher = {University of Chicago Press},
  location = {Chicago, IL},
  url = {https://press.uchicago.edu/ucp/books/book/chicago/E/bo185169296.html},
  urldate = {2024-09-27},
  abstract = {An engaging argument about what experimental music can tell us about being human. In Experimenting the Human, G Douglas Barrett argues that experimental music speaks to the contemporary posthuman, a condition in which science and technology decenter human agency amid the uneven temporality of postwar global capitalism. Time moves forward for some during this period, while it seems to stand still or even move backward for others. Some say we’re already posthuman, while others endure the extended consequences of never having been considered fully human in the first place. Experimental music reflects on this state, Barrett contends, through its interdisciplinary involvements in postwar science, technology, and art movements. Rather than pursuing the human's beyond, experimental music addresses the social and technological conditions that support such a pursuit. Barrett locates this tendency of experimentalism throughout its historical entanglements with cybernetics, and in his intimate analysis of Alvin Lucier’s neurofeedback music, Pamela Z’s BodySynth performances, Nam June Paik’s musical robotics, Pauline Oliveros’s experiments with radio astronomy, and work by Laetitia Sonami, Yasunao Tone, and Jerry Hunt. Through a unique meeting of music studies, media theory, and art history, Experimenting the Human provides fresh insights into what it means to be human.},
  isbn = {978-0-226-82340-9},
  langid = {english},
  pagetotal = {240},
  keywords = {artificial intelligence,contemporary art,cybernetics,experimental music,indeterminacy,media studies,music technology,political economy,posthumanism,postwar era}
}

@book{barrettSoundCriticalMusic2016,
  title = {After {{Sound}}: {{Toward}} a {{Critical Music}}},
  shorttitle = {After {{Sound}}},
  author = {Barrett, G. Douglas},
  date = {2016-08-11},
  edition = {1st edition},
  publisher = {Bloomsbury Academic},
  abstract = {After Sound considers contemporary art practices that reconceive music beyond the limitation of sound. This book is called After Sound because music and sound are, in Barrett's account, different entities. While musicology and sound art theory alike typically equate music with pure instrumental sound, or absolute music, Barrett posits music as an expanded field of artistic practice encompassing a range of different media and symbolic relationships. The works discussed in After Sound thus use performance, text scores, musical automata, video, social practice, and installation while they articulate a novel aesthetic space for a radically engaged musical practice. Coining the term "critical music," this book examines a diverse collection of art projects which intervene into specific political and philosophical conflicts by exploring music's unique historical forms. Through a series of intimate studies of artworks surveyed from the visual and performing arts of the past ten years-Pussy Riot, Ultra-red, Hong-Kai Wang, Peter Ablinger, Pauline Boudry and Renate Lorenz, and others-After Sound offers a significant revision to the way we think about music. The book as a whole offers a way out of one of the most vexing deadlocks of contemporary cultural criticism: the choice between a sound art effectively divorced from the formal-historical coordinates of musical practice and the hermetic music that dominates new music circles today.},
  langid = {english},
  pagetotal = {333}
}

@inproceedings{baumannBodyMovementSonification2018,
  title = {Body Movement Sonification Using the Web Audio {{API}}},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Baumann, Christian and Friederike, Johanna and Milde, Jan-Torsten},
  editor = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
  date = {2018-09},
  series = {Wac '18},
  publisher = {TU Berlin},
  location = {Berlin, Germany},
  issn = {2663-5844},
  abstract = {In this paper we describe the ongoing research on the development of a body movement sonification system. High precision, high resolution wireless sensors are used to track the body movement and record muscle excitation. We are currently using 6 sensors. In the final version of the system full body tracking can be achieved. The recording system provides a web server including a simple REST API, which streams the recorded data in JSON format. An intermediate proxy server pre-processes the data and transmits it to the final sonification system. The sonification system is implemented using the web audio api. We are experimenting with a set of different sonification strategies and algorithms. Currently we are testing the system as part of an interactive, guided therapy, establishing additional acoustic feedback channels for the patient. In a second stage of the research we are going to use the system in a more musical and artistic way. More specifically we plan to use the system in cooperation with a violist, where the acoustic feedback channel will be integrated into the performance.}
}

@inproceedings{bernardoAudioWorkletbasedSignalEngine2019,
  title = {An {{AudioWorklet-based}} Signal Engine for a Live Coding Language Ecosystem},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Bernardo, Francisco and Kiefer, Chris and Magnusson, Thor},
  editor = {Xambó, Anna and Martín, Sara R. and Roma, Gerard},
  date = {2019-12},
  series = {Wac '19},
  pages = {77--82},
  publisher = {NTNU},
  location = {Trondheim, Norway},
  issn = {2663-5844},
  abstract = {This paper reports on early advances in the design of an ecosystem for creating new live coding languages, optimal for audio synthesis, machine learning and machine listening. We present the design rationale and challenges when applying the Web Audio API, and in particular, an AudioWorklet-based solution to refactoring our digital signal processing library Maximilian.js for our high-performance signal synthesis engine. Furthermore, we contribute with a new system implementation, engineered for modern web applications, and for the live coding community to design their own idiosyncratic languages and interfaces applying our signal engine. The evaluation shows that the system runs with high reliability, efficiency and low latency.}
}

@article{bevilacquaDesigningComposingPerforming2021,
  title = {On {{Designing}}, {{Composing}} and {{Performing Networked Collective Interactions}}},
  author = {Bevilacqua, Frederic and Matuszewski, Benjamin and Paine, Garth and Schnell, Norbert},
  date = {2021},
  journaltitle = {Organised sound : an international journal of music technology},
  volume = {26},
  number = {3},
  pages = {333--339},
  publisher = {Cambridge University Press},
  location = {Cambridge, UK},
  issn = {1355-7718},
  doi = {10.1017/S135577182100042X},
  abstract = {In this article, we discuss some of our research with Local Area Networks (LAN) in the context of sound installations or musical performances. Our systems, built on top of Web technologies, enable novel possibilities of collective and collaborative interaction, in particular by simplifying public access to the artwork by presenting the work through the web browser of their smartphone/tablet. Additionally, such a technical framework can be extended with so-called nano-computers, microprocessors and sensors. The infrastructure is completely agnostic as to how many clients are attached, or how they connect, which means that if the work is available in a public space, groups of friends, or even informally organised flash mobs, may engage with the work and perform the contents of the work at any time, and if available over the Internet, at any place. More than the technical details, the specific artistic directions or the supposed autonomy of the agents of our systems, this article focuses on how such ‘networks of devices’ interleave with the ‘network of humans’ composed of the people visiting the installation or participating in the concert. Indeed, we postulate that an important point in understanding and describing such proposals is to consider the relation between these two networks, the way they co-exist and entangle themselves through perception and action. To exemplify these ideas, we present a number of case studies, sound installations and concert works, very different in scope and artistic goal, and examine how this interaction is materialised from several standpoints.},
  langid = {english},
  keywords = {Actor-network theory,Audiences,Browsers (Computer programs),Communication,Computer networks,Computer science,Human-computer interaction,Local Area Networks,Microcomputers,Microprocessors,Music,Public spaces,Research,Smartphones,Sound}
}

@inproceedings{binMuseumBrowserTranslating2019,
  title = {From the Museum to the Browser: {{Translating}} a Music-Driven Exhibit from Physical Space to a Web App},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Bin, S. M. Astrid and Bui, Christina and Genchel, Benjamin and Sali, Kaushal and Magerko, Brian and Freeman, Jason},
  editor = {Xambó, Anna and Martín, Sara R. and Roma, Gerard},
  date = {2019-12},
  series = {Wac '19},
  pages = {24--29},
  publisher = {NTNU},
  location = {Trondheim, Norway},
  issn = {2663-5844},
  abstract = {This paper describes the process of developing a browser-based version of GrooveMachine, a tangible museum exhibit that aims to foster interest in computer science (CS) through the music-driven exploration of a computational system. GrooveMachine is aimed at kids aged 10-14, and specifically targets learners from from groups currently under-represented in computing by demonstrating CS applications that challenge stereotypes. While an observational study suggests that GrooveMachine triggers situational interest, long-term engagement with CS requires this interest to be deepened and developed. To provide an opportunity for interest development, we have implemented a browser-based GrooveMachine. This not only offers the opportunity for learners to continue their exploration of CS through creative interaction, but provides a pathway to other music and CS learning platforms where they can deepen this interest. In this paper we describe the theoretical underpinnings of interest, how it relates to CS, and how it intersects with identity. We also describe the differences between the museum and browser contexts. We detail the design and implementation of GrooveMachine in the museum and explain how we translated it to the browser, including the rationale behind our central design decisions and a discussion of our technical implementation. In this way we provide valuable insight for researchers who want to reach larger audiences by developing browser-based versions of physical installations.}
}

@article{blaauwNeuralParametricSinging2017,
  title = {A {{Neural Parametric Singing Synthesizer Modeling Timbre}} and {{Expression}} from {{Natural Songs}}},
  author = {Blaauw, Merlijn and Bonada, Jordi},
  date = {2017},
  journaltitle = {Applied sciences},
  volume = {7},
  number = {12},
  pages = {1313-},
  publisher = {MDPI AG},
  location = {Basel},
  issn = {2076-3417},
  doi = {10.3390/app7121313},
  abstract = {We recently presented a new model for singing synthesis based on a modified version of the WaveNet architecture. Instead of modeling raw waveform, we model features produced by a parametric vocoder that separates the influence of pitch and timbre. This allows conveniently modifying pitch to match any target melody, facilitates training on more modest dataset sizes, and significantly reduces training and generation times. Nonetheless, compared to modeling waveform directly, ways of effectively handling higher-dimensional outputs, multiple feature streams and regularization become more important with our approach. In this work, we extend our proposed system to include additional components for predicting F0 and phonetic timings from a musical score with lyrics. These expression-related features are learned together with timbrical features from a single set of natural songs. We compare our method to existing statistical parametric, concatenative, and neural network-based approaches using quantitative metrics as well as listening tests.},
  langid = {english},
  keywords = {Acoustics,Machine learning,Modeling,Phonetics,Pitch,Quality,Singers,Singing,Songs}
}

@article{boutwellLeagueAutomaticMusic2009a,
  title = {The {{League}} of {{Automatic Music Composers}}, 1978–1983. {{With John Bischoff}}, {{Jim Horton}}, {{Tim Perkis}}, {{David Behrman}}, {{Paul DeMarinis}}, and {{Rich Gold}}. {{New World Records}} 80671-2, 2007.},
  author = {Boutwell, Brett},
  date = {2009-05},
  journaltitle = {Journal of the Society for American Music},
  volume = {3},
  number = {2},
  pages = {263--264},
  issn = {1752-1971, 1752-1963},
  doi = {10.1017/S1752196309091147},
  url = {https://www.cambridge.org/core/journals/journal-of-the-society-for-american-music/article/league-of-automatic-music-composers-19781983-with-john-bischoff-jim-horton-tim-perkis-david-behrman-paul-demarinis-and-rich-gold-new-world-records-806712-2007/88BD104DDE3FA3EF1AF1FD6095A558B2},
  urldate = {2024-09-22},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS1752196309091147/resource/name/firstPage-S1752196309091147a.jpg},
  langid = {english}
}

@inproceedings{bownCreativeCodingInteraction2020,
  title = {Creative {{Coding}} and {{Interaction Design}} for {{Media Multiplicities}}: {{Challenges}}, {{Paradigms}} and {{Frameworks}}},
  shorttitle = {Creative {{Coding}} and {{Interaction Design}} for {{Media Multiplicities}}},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Tangible}}, {{Embedded}}, and {{Embodied Interaction}}},
  author = {Bown, Oliver and Fraietta, Angelo and Loke, Lian and Ferguson, Sam},
  date = {2020-02-09},
  series = {{{TEI}} '20},
  pages = {877--880},
  publisher = {Association for Computing Machinery},
  location = {Sydney NSW, Australia},
  doi = {10.1145/3374920.3374966},
  url = {https://doi.org/10.1145/3374920.3374966},
  urldate = {2020-06-22},
  abstract = {Media multiplicities are media artworks that employ multiple networked digital devices to create holistic aesthetic effects. Examples include the networked light artworks of Squidsoup, the Spaxels drone-mounted light performances, DrawBots, Siftables and many others. In multiplicitous media artworks, each individual device is a programmable node connected to other nodes via a network connection, and may combine any number of sensors and actuators. A number of development technologies support artists and designers to configure and create media multiplicities, but this domain offers new challenges for creative practitioners. This workshop aims to bring together experts in creative coding and interaction design to discuss and conceptualise frameworks for the practice of media multiplicities. Open challenges include: speed of setup; ease of hardware configuration; speed of code deployment; ability to model and simulate works in VR; network connectivity and stability; and understanding network, computation and power constraints.},
  isbn = {978-1-4503-6107-1},
  keywords = {distributed audio,embedded computing,internet of things,media multiplicities,physical computing}
}

@article{bownSupportingCreativePractice2021,
  title = {Supporting {{Creative Practice}} in {{Wireless Distributed Sound Installations Given Technical Constraints}}},
  author = {Bown, O. and Ferguson, S. and Dos Santos, A. D. P. and Mikolajczyk, K.},
  date = {2021-10-01},
  volume = {69},
  number = {10},
  pages = {757--767},
  publisher = {Audio Engineering Society},
  issn = {1549-4950},
  doi = {10.17743/jaes.2021.0039},
  url = {http://hdl.handle.net/1959.4/unsworks_79137},
  urldate = {2024-09-22},
  abstract = {In this paper we present creative practice-led research into building large, scalable “multiplicitous media” artworks in which many networked devices control lights and speakers and are coordinated over Wi-Fi to create holistic artistic and environmental experiences. We discuss competing constraints, in particular the creative constraints associated with the challenge of coding complex multi-device behaviors, maximizing creative freedom and simplifying complex engineering and design decisions. Based on recent experience building multi-device digital installation works, we propose an approach, the “broadcast-first recipe, ” that aims to simplify the space of creative possibilities, with a trade-off between expressive power and creative efficiency that we argue is worth adopting. We examine this approach in light of hard technical constraints such as central processing unit (CPU) and Wi-Fi bandwidth budgets, which we discuss in a concrete example. We consider how the effectiveness of the proposed approach could be further leveraged in the provision of support tools.}
}

@article{braidottiTheoreticalFrameworkCritical2019,
  title = {A {{Theoretical Framework}} for the {{Critical Posthumanities}}},
  author = {Braidotti, Rosi},
  date = {2019},
  journaltitle = {Theory, culture \& society},
  volume = {36},
  number = {6},
  pages = {31--61},
  publisher = {SAGE Publications},
  location = {London, England},
  issn = {0263-2764},
  doi = {10.1177/0263276418771486},
  abstract = {What are the parameters that define a posthuman knowing subject, her scientific credibility and ethical accountability? Taking the posthumanities as an emergent field of enquiry based on the convergence of posthumanism and post-anthropocentrism, I argue that posthuman knowledge claims go beyond the critiques of the universalist image of ‘Man’ and of human exceptionalism. The conceptual foundation I envisage for the critical posthumanities is a neo-Spinozist monistic ontology that assumes radical immanence, i.e. the primacy of intelligent and self-organizing matter. This implies that the posthuman knowing subject has to be understood as a relational embodied and embedded, affective and accountable entity and not only as a transcendental consciousness. Two related notions emerge from this claim: firstly, the mind-body continuum – i.e. the embrainment of the body and embodiment of the mind – and secondly, the nature-culture continuum – i.e. ‘naturecultural’ and ‘humanimal’ transversal bonding. The article explores these key conceptual and methodological perspectives and discusses the implications of the critical posthumanities for practices in the contemporary ‘research’ university.},
  langid = {english},
  keywords = {Consciousness,Convergence,Ethics,Exceptionalism,Liability (Law),Mind and body,Ontology,Posthumanism,Radicalism,Truthfulness and falsehood}
}

@book{brandomArticulatingReasonsIntroduction2009,
  title = {Articulating {{Reasons}}: {{An Introduction}} to {{Inferentialism}}},
  shorttitle = {Articulating {{Reasons}}},
  author = {Brandom, Robert},
  date = {2009},
  edition = {1},
  publisher = {Harvard University Press},
  location = {Cambridge},
  doi = {10.4159/9780674028739},
  abstract = {Robert B. Brandom is one of the most original philosophers of our day, whose book Making It Explicit covered and extended a vast range of topics in metaphysics, epistemology, and philosophy of language--the very core of analytic philosophy. This new work provides an approachable introduction to the complex system that Making It Explicit mapped out. A tour of the earlier book's large ideas and relevant details, Articulating Reasons offers an easy entry into two of the main themes of Brandom's work: the idea that the semantic content of a sentence is determined by the norms governing inferences to and from it, and the idea that the distinctive function of logical vocabulary is to let us make our tacit inferential commitments explicit. Brandom's work, making the move from representationalism to inferentialism, constitutes a near-Copernican shift in the philosophy of language--and the most important single development in the field in recent decades. Articulating Reasons puts this accomplishment within reach of nonphilosophers who want to understand the state of the foundations of semantics.Table of Contents: Introduction 1\textbackslash. Semantic Inferentialism and Logical Expressivism 2\textbackslash. Action, Norms, and Practical Reasoning 3\textbackslash. Insights and Blindspots of Reliabilism 4\textbackslash. What Are Singular Terms, and Why Are There Any? 5\textbackslash. A Social Route from Reasoning to Representing 6\textbackslash. Objectivity and the Normative Fine Structure of Rationality Notes Index Displaying a sovereign command of the intricate discussion in the analytic philosophy of language, Brandom manages successfully to carry out a program within the philosophy of language that has already been sketched by others, without losing sight of the vision inspiring the enterprise in the important details of his investigation ' Using the tools of a complex theory of language, Brandom succeeds in describing convincingly the practices in which the reason and autonomy of subjects capable of speech and action are expressed. \textbackslash --J'rgen Habermas},
  isbn = {978-0-674-02873-9},
  langid = {english},
  keywords = {Communication,Expression (Philosophy),Inference,Language,Language and languages,Language and logic,Logic,Philosophy,Reasoning,Semantics,Semantics (Philosophy)}
}

@article{brusseauEthicsFrameworkInternet2024,
  title = {An {{Ethics Framework}} for the {{Internet}} of {{Musical Things}}},
  author = {Brusseau, James and Turchet, Luca},
  date = {2024},
  journaltitle = {IEEE transactions on technology and society},
  pages = {1--1},
  publisher = {IEEE},
  issn = {2637-6415},
  doi = {10.1109/TTS.2024.3398423},
  abstract = {The Internet of Musical Things (IoMusT) is an emerging field of academic and industrial research that extends the Internet of Things to the musical domain. Scarce research has been conducted on the fields ethical aspects, and to fill this gap we propose a framework for the ethical design and evaluation of IoMusT technologies and applications. Besides being ethically rigorous, the framework seeks to be accessible for information engineers, musicians, and the wider circle of participants in the IoMusT. The purpose is to facilitate and quicken the process of ethically designing and evaluating work at the intersection of network-based technology and musical creativity. Finally, we exemplify the framework by applying it to an IoMusT experimental performance. Beyond facilitating the ethical evaluation of technologically enhanced music, the framework also advances work in contemporary AI ethics in two ways. First, by introducing the principles of creativity and decentralization as critical to ethically exploring musical creativity. Second, by organizing the principles of AI ethics under a human-centric logic.},
  langid = {english},
  keywords = {Artificial intelligence,Ethics,Instruments,Internet of things,Music,Privacy}
}

@online{buntingCybercafeNetArt1994,
  title = {Cybercafe {{Net Art Projects}}.},
  author = {Bunting, Heath},
  date = {1994},
  url = {https://irational.org/cybercafe/},
  urldate = {2024-09-28}
}

@inproceedings{burchettComposingImprovisingUsing2021,
  title = {Composing and Improvising Using Sound Content-Based Descriptive Filtering},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Burchett, Dylan and Thompson, William and Franklin, Austin A},
  editor = {Joglar-Ongay, Luis and Serra, Xavier and Font, Frederic and Tovstogan, Philip and Stolfi, Ariane and A. Correya, Albin and Ramires, Antonio and Bogdanov, Dmitry and Faraldo, Angel and Favory, Xavier},
  date = {2021-07},
  series = {Wac '21},
  publisher = {UPF},
  location = {Barcelona, Spain},
  issn = {2663-5844},
  abstract = {The Freesound Player is a digital instrument developed in Max MSP that uses the Freesound API to make requests for as many as 16 sound samples which are filtered based on sound content. Once the samples are returned they are loaded into buffers and can be performed using a MIDI controller and processed in a variety of ways. The filters implemented will be discussed and demonstrated using three music compositions by the authors, along with considerations for composing and improvising using sound content-based descriptive filtering.}
}

@inproceedings{cakmakComposingSpatialMusic2019,
  title = {Composing Spatial Music with Web Audio and {{WebVR}}},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Çakmak, Cem and Hamilton, Rob},
  editor = {Xambó, Anna and Martín, Sara R. and Roma, Gerard},
  date = {2019-12},
  series = {Wac '19},
  pages = {19--23},
  publisher = {NTNU},
  location = {Trondheim, Norway},
  issn = {2663-5844},
  abstract = {Composers have been exploring complex spatialization techniques within multi-channel sound fields since the earliest days of electroacoustic and electronic music. However the reproduction of such works outside of highly specified concert halls and academic research facilities, or even their accurate reproduction within those spaces, is difficult and unpredictable at best. Tools such as Omnitone combine the reach and simplicity of web browsers with the flexibility and power of higher-order ambisonics (HOA) and binaural rendering, ensuring greater accessibility for existing spatial electronic musical works as well as acting as a platform upon which future works for virtual sound fields can be implemented. This paper describes the technical design and artistic conception of one such spatial composition for binaural listening and immersive visuals on the web - “od” - produced in the CRAIVE-Lab, an immersive audio-visual facility.}
}

@inproceedings{carotNetworkedMusicPerformance2007,
  title = {Networked Music Performance: {{AES}} 30th {{International Conference}}},
  shorttitle = {Networked Music Performance},
  author = {Carôt, Alexander and Renaud, Alain B. and Rebelo, Pedro},
  date = {2007-12-01},
  url = {http://www.scopus.com/inward/record.url?scp=45349095824&partnerID=8YFLogxK},
  urldate = {2024-09-22},
  abstract = {The fast paced development of broadband internet infrastructure makes high quality real time networked music performances possible. In recent years, the implementation of over-provisioned network backbones has led to the development of several initiatives that allow real-time or time-shifted interactions of geographically displaced musicians. The development of such networks is also leading to the creation of new forms of art and communications that use the network as a core.}
}

@inproceedings{carsonImmaterialcloudUsingPeerpeer2021,
  title = {Immaterial.Cloud: {{Using}} Peer-to-Peer Technologies for Music},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Carson, Tate},
  editor = {Joglar-Ongay, Luis and Serra, Xavier and Font, Frederic and Tovstogan, Philip and Stolfi, Ariane and A. Correya, Albin and Ramires, Antonio and Bogdanov, Dmitry and Faraldo, Angel and Favory, Xavier},
  date = {2021-07},
  series = {Wac '21},
  publisher = {UPF},
  location = {Barcelona, Spain},
  issn = {2663-5844},
  abstract = {immaterial.cloud is an immersive audiovisual installation that explores a possible networked future of peer-to-peer technologies, away from cloud computing. Participants experience the work via two to four smartphones placed in different locations in a room. As participants walk up to a phone, they see a representation of themselves through data. If the participant gets close enough, the phone triggers a change in the sound of immaterial.cloud and the other phones follow.}
}

@inproceedings{carsonMorePerfectUnion2018,
  title = {A More Perfect Union: {{Composition}} with Audience-Controlled Smartphone Speaker Array and Evolutionary Computer Music},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Carson, Tate},
  editor = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
  date = {2018-09},
  series = {Wac '18},
  publisher = {TU Berlin},
  location = {Berlin, Germany},
  issn = {2663-5844},
  abstract = {A more perfect union incorporates an audience-controlled smartphone speaker array with evolutionary computer music. A genetic algorithm drives the work and the performance practice that the audience follows.}
}

@inproceedings{carsonSoundsAwareMobile2019,
  title = {Sounds Aware: A Mobile App for Raising Awareness of Environmental Sound},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Carson, Tate},
  editor = {Xambó, Anna and Martín, Sara R. and Roma, Gerard},
  date = {2019-12},
  series = {Wac '19},
  pages = {14--18},
  publisher = {NTNU},
  location = {Trondheim, Norway},
  issn = {2663-5844},
  abstract = {Sounds Aware is a web application that runs on a smartphone and uses machine learning to detect human-made sound (anthrophony) and masks it with ambient music as a user walks around their environment. A study was completed to determine if this app is an effective means of shifting a user's attention away from anthrophony and to biological (biophony) and geophysical (geophony) sounds while walking and encouraging environmental awareness. Though the model is pre-trained with the author's local environmental sounds, the user can train the model further on their unique soundscape so that each user gets a personalized experience. After the training process, the user can listen to ambient music based on traits of the surrounding anthrophony. If the app senses less anthrophony and more biophony or geophony, then the music fades away, bringing the user's attention to the anthrophony.}
}

@inproceedings{clesterComposingNetworkStreams2021,
  title = {Composing the {{Network}} with {{Streams}}},
  booktitle = {Proceedings of the 16th {{International Audio Mostly Conference}}},
  author = {Clester, Ian and Freeman, Jason},
  date = {2021-10-15},
  series = {{{AM}} '21},
  pages = {196--199},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3478384.3478416},
  url = {https://dl.acm.org/doi/10.1145/3478384.3478416},
  urldate = {2024-09-21},
  abstract = {We present Aleatora, an early-stage framework for building compositions from lazy, effectful streams. Aleatora’s streams, which may be combined by sequential, parallel, or functional composition, are well-suited to expressing interactive and aleatoric musical compositions. Aleatora includes a networking module which aids in writing compositions for the Internet of Sounds using network data sources such as OSC, external APIs, and Internet sound repositories (e.g. Freesound). This paper describes the design and implementation of Aleatora and demonstrates how it can facilitate weaving external input sources, such as network streams, into compositions.},
  isbn = {978-1-4503-8569-5}
}

@inproceedings{clesterKilobeatLowlevelCollaborative2021,
  title = {Kilobeat: Low-Level Collaborative Livecoding},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Clester, Ian J},
  editor = {Joglar-Ongay, Luis and Serra, Xavier and Font, Frederic and Tovstogan, Philip and Stolfi, Ariane and A. Correya, Albin and Ramires, Antonio and Bogdanov, Dmitry and Faraldo, Angel and Favory, Xavier},
  date = {2021-07},
  series = {Wac '21},
  publisher = {UPF},
  location = {Barcelona, Spain},
  issn = {2663-5844},
  abstract = {This paper presents kilobeat, a collaborative, web-based, DSP-oriented livecoding platform. Players make music together by writing short snippets of code. Inspired by the practices of bytebeat, these snippets are low-level expressions representing digital audio signals. Unlike existing platforms, kilobeat does not adapt existing livecoding languages or introduce a new one: players write JavaScript expressions (using standard operators and math functions) to generate samples directly. This approach reduces the amount of background knowledge required to understand players' code and makes kilobeat amenable to synthesis and DSP pedagogy. To facilitate collaboration, players can hear each other's audio (distributed spatially in a virtual room), see each other's code (including edits and run actions), and depend on each other's output (by referencing other players as variables). Additionally, performances may be recorded and replayed later, including all player actions. For accessibility and ease of sharing, kilobeat is built on Web Audio and WebSockets.}
}

@incollection{comanducciIntelligentNetworkedMusic2023,
  title = {Intelligent {{Networked Music Performance Experiences}}},
  booktitle = {Special {{Topics}} in {{Information Technology}}},
  author = {Comanducci, Luca},
  editor = {Riva, Carlo G.},
  date = {2023},
  pages = {119--130},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-15374-7_10},
  url = {https://doi.org/10.1007/978-3-031-15374-7_10},
  urldate = {2024-09-19},
  abstract = {A Networked Music Performance (NMP) is defined as what happens when geographically displaced musicians interact together while connected via network. The first NMP experiments begun in the 1970s, however, only recently the development of network communication technologies has created the necessary infrastructure needed to successfully create an NMP. Moreover, the widespread adoption of network-based interactions during the COVID-19 pandemic has generated a renewed interest towards distant music-based interaction. In this chapter we present the Intelligent networked Music PERforMANce experiENCEs (IMPERMANENCE) as a comprehensive NMP framework that aims at creating a compelling performance experience for the musicians. In order to do this, we first develop the neTworkEd Music PErfoRmANCe rEsearch (TEMPERANCE) framework in order to understand which are the main needs of the participants in a NMP. Informed by these results we then develop IMPERMANENCE accordingly.},
  isbn = {978-3-031-15374-7},
  langid = {english},
  keywords = {Deep learning,Microphone array,Networked music performance,Spatial audio}
}

@inproceedings{correyaEssentiaTensorFlowModels2021,
  title = {Essentia {{TensorFlow}} Models for Audio and Music Processing on the Web},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Correya, Albin and Alonso-Jiménez, Pablo and Marcos-Fernández, Jorge and Serra, Xavier and Bogdanov, Dmitry},
  editor = {Joglar-Ongay, Luis and Serra, Xavier and Font, Frederic and Tovstogan, Philip and Stolfi, Ariane and A. Correya, Albin and Ramires, Antonio and Bogdanov, Dmitry and Faraldo, Angel and Favory, Xavier},
  date = {2021-07},
  series = {Wac '21},
  publisher = {UPF},
  location = {Barcelona, Spain},
  issn = {2663-5844},
  abstract = {Recent advances in web-based machine learning (ML) tools empower a wide range of application developers in both industrial and creative contexts. The availability of pre-trained ML models and JavaScript (JS) APIs in frameworks like TensorFlow.js enabled developers to use AI technologies without demanding domain expertise. Nevertheless, there is a lack of pre-trained models in web audio compared to other domains, such as text and image analysis. Motivated by this, we present a collection of open pre-trained TensorFlow.js models for music-related tasks on the Web. Our models currently allow for different types of music classification (e.g., genres, moods, danceability, voice or instrumentation), tempo estimation, and music feature embeddings. To facilitate their use, we provide a dedicated JS add-on module essentia.js-model within the Essentia.js library for audio and music analysis. It has a simple API, enabling end-to-end analysis from audio input to prediction results on web browsers and Node.js. Along with the Web Audio API and web workers, it can also be used to build real-time applications. We provide usage examples, discuss possible use-cases, and report benchmarking results.}
}

@inproceedings{crettiWebWallWhispers2018,
  title = {Web {{Wall Whispers}}: An Interactive Web-Based Sound Work},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Cretti, Francesco and Morino, Luca and Liuni, Marco and Gervasoni, Stefano and Agostini, Andrea and Servetti, Antonio},
  editor = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
  date = {2018-09},
  series = {Wac '18},
  publisher = {TU Berlin},
  location = {Berlin, Germany},
  issn = {2663-5844},
  abstract = {Web Wall Whispers (www) is an interactive sound work that heavily relies on the web audio technology to enable a virtual high-quality multimodal exploration of a monumental mural. The user's navigation through the artwork generates a unique interactive musical composition at every access, in a challenging paradigm of open form based on a virtual dialogue between the visitors and the composer. The project is conceived as a part of the Segni per la Speranza (spls, Signs for Hope) multimodal artwork, a project aimed at the reappraisal of urban outlying areas. All the constituent materials are freely distributed under the open source GNU General Public Licence, thus allowing the build-up of extensions or new versions of this multimodal artwork paradigm.}
}

@article{dannenbergCommunicationRealTimeMusic2021,
  title = {Communication for {{Real-Time Music Systems}}: {{An Overview}} of {{O2}}},
  shorttitle = {Communication for {{Real-Time Music Systems}}},
  author = {Dannenberg, Roger B.},
  date = {2021},
  journaltitle = {Computer music journal},
  volume = {45},
  number = {4},
  pages = {7--19},
  publisher = {MIT Press},
  location = {One Rogers Street, Cambridge, MA 02142-1209, USA},
  issn = {0148-9267},
  doi = {10.1162/comj_a_00620},
  abstract = {Message passing between processes and across networks offers a powerful method to integrate and coordinate various music programs, facilitating software reuse, modularity, and parallel processing. Networking can integrate components that use different languages and hardware. In this article we describe O2, a flexible protocol for communication ranging from the thread level up to the level of global networks. Messages in O2 are similar to those of Open Sound Control, but O2 offers many additional features, including discovery, clock synchronization, a reliable message delivery option, and routing based on services rather than specific network addresses. A bridge mechanism extends the reach of O2 to web browsers, shared memory threads, and small microcontrollers. The design, implementation, and applications of O2 are described.},
  langid = {english}
}

@article{dannenbergO2NetworkProtocol2019,
  title = {O2: {{A Network Protocol}} for {{Music Systems}}},
  shorttitle = {O2},
  author = {Dannenberg, Roger B.},
  namea = {Fontana, Federico and {Federico Fontana}},
  nameatype = {collaborator},
  date = {2019},
  journaltitle = {Wireless communications and mobile computing},
  volume = {2019},
  number = {2019},
  pages = {1--12},
  publisher = {Hindawi Publishing Corporation},
  location = {Cairo, Egypt},
  issn = {1530-8669},
  doi = {10.1155/2019/8424381},
  abstract = {O2 is a communication protocol for music systems that extends and interoperates with the popular Open Sound Control (OSC) protocol. Many computer musicians routinely deal with problems of interconnection, unreliable message delivery, and clock synchronization. O2 solves these problems, offering named services, automatic network address discovery, clock synchronization, and a reliable message delivery option, as well as interoperability with existing OSC libraries and applications. Aside from these new features, O2 owes much of its design to OSC, making it easy to migrate existing OSC applications to O2 or for developers familiar with OSC to begin using O2. O2 addresses the problems of interprocess communication within distributed music applications.},
  langid = {english},
  keywords = {Communication,Software}
}

@video{elkSENSUSSmartGuitar2016,
  entrysubtype = {video},
  title = {{{SENSUS Smart Guitar Performance}}},
  editor = {{Elk}},
  editortype = {director},
  date = {2016-05-04},
  url = {https://www.youtube.com/watch?v=fqzEQnsSIoY},
  urldate = {2024-09-27},
  abstract = {The video shows the full expressive potential of SENSUS Smart Guitar, which allows guitar players to use infinite modulations and effects, switching between electro-acoustic and electronic sounds in real time and in a natural way. All sounds and modulations can be obtained without the help of any accessory or computer: 100\% SENSUS Smart Guitar sound.  The music in the video is produced and performed live by Valerio Fuiano, professional guitar player and music producer.}
}

@misc{finnIntroductionTimeSensitiveNetworking2022,
  title = {Introduction to {{Time-Sensitive Networking}}},
  author = {Finn, Norman},
  date = {2022},
  journaltitle = {IEEE communications standards magazine},
  volume = {6},
  number = {4},
  pages = {8--13},
  issn = {2471-2825},
  doi = {10.1109/MCOMSTD.0004.2200046},
  abstract = {Since the year 2000, a number of companies and standards development organizations have been producing products and standards for Time-Sensitive Networking to support real-time applications that require zero packet loss due to buffer congestion, extremely low packet loss due to equipment failure, and guaranteed upper bounds on end-to-end latency. Often, a robust capability for time synchronization to less than 1 s is also required. These networks consist of specially-featured bridges that are interconnected using standard Ethernet links with standard MAC/PHY layers. Since the year 2012, this technology has advanced to the use of routers, as well as bridges, and features of interest to Time-Sensitive Net-working have been added to both Ethernet and wireless standards. Since the year 2018, TSN standardization has been expanding to include more queuing and pacing technologies, and to support new markets, such as industrial, automotive, aviation, and service provider applications.},
  langid = {english},
  organization = {IEEE},
  keywords = {Bridges,Business enterprises}
}

@book{fisherCapitalistRealismThere2022,
  title = {Capitalist {{Realism}}: {{Is}} There No Alternative?},
  shorttitle = {Capitalist {{Realism}}},
  author = {Fisher, Mark},
  date = {2022-11-25},
  publisher = {Zero Books},
  location = {Winchester, UK Washington, USA},
  abstract = {An analysis of the ways in which capitalism has presented itself as the only realistic political-economic system.},
  isbn = {978-1-84694-317-1},
  langid = {english},
  pagetotal = {120}
}

@book{fisherPostcapitalistDesireFinal2020,
  title = {Postcapitalist {{Desire}}: {{The Final Lectures}}},
  shorttitle = {Postcapitalist {{Desire}}},
  author = {Fisher, Mark},
  editor = {Colquhoun, Matt},
  date = {2020-09-22},
  publisher = {Repeater},
  abstract = {A collection of transcripts from Mark Fisher's final series of lectures at Goldsmiths, University of London, in late 2016.Edited with an introduction by Matt Colquhoun, this collection of lecture notes and transcriptions reveals acclaimed writer and blogger Mark Fisher in his element -- the classroom -- outlining a project that Fisher's death left so bittersweetly unfinished.Beginning with that most fundamental of questions -- "Do we really want what we say we want?" -- Fisher explores the relationship between desire and capitalism, and wonders what new forms of desire we might still excavate from the past, present, and future. From the emergence and failure of the counterculture in the 1970s to the continued development of his left-accelerationist line of thinking, this volume charts a tragically interrupted course for thinking about the raising of a new kind of consciousness, and the cultural and political implications of doing so.For Fisher, this process of consciousness raising was always, fundamentally, psychedelic -- just not in the way that we might think...},
  langid = {english},
  pagetotal = {252}
}

@article{foukasNetworkSlicing5G2017,
  title = {Network {{Slicing}} in {{5G}}: {{Survey}} and {{Challenges}}},
  shorttitle = {Network {{Slicing}} in {{5G}}},
  author = {Foukas, Xenofon and Patounas, Georgios and Elmokashfi, Ahmed and Marina, Mahesh K.},
  date = {2017-05},
  journaltitle = {IEEE Communications Magazine},
  volume = {55},
  number = {5},
  pages = {94--100},
  issn = {1558-1896},
  doi = {10.1109/MCOM.2017.1600951},
  url = {https://ieeexplore.ieee.org/document/7926923},
  urldate = {2024-09-19},
  abstract = {5G is envisioned to be a multi-service network supporting a wide range of verticals with a diverse set of performance and service requirements. Slicing a single physical network into multiple isolated logical networks has emerged as a key to realizing this vision. This article is meant to act as a survey, the first to the authors' knowledge, on this topic of prime interest. We begin by reviewing the state of the art in 5G network slicing and present a framework for bringing together and discussing existing work in a holistic manner. Using this framework, we evaluate the maturity of current proposals and identify a number of open research questions.},
  eventtitle = {{{IEEE Communications Magazine}}},
  keywords = {5G mobile communication,Cloud computing,Computer architecture,Market opportunities,Mobile computing}
}

@inproceedings{fraiettaTransparentCommunicationMultiplicities2020,
  title = {Transparent {{Communication Within Multiplicities}}},
  booktitle = {2020 27th {{Conference}} of {{Open Innovations Association}} ({{FRUCT}})},
  author = {Fraietta, Angelo and Bown, Oliver and Ferguson, Sam},
  date = {2020},
  volume = {27},
  number = {1},
  pages = {61--72},
  publisher = {FRUCT},
  issn = {2305-7254},
  doi = {10.23919/FRUCT49677.2020.9210989},
  abstract = {The Internet of Musical Things is an emerging field of research that intersects the Internet of Things, humancomputer interaction, ubiquitous music, artificial intelligence, gaming, virtual reality and participatory art through device multiplicity. This paper introduces a paradigm whereby data points and variable parameters can be strategically mapped or bound using aliases, data types and scoping as an alternative to flat address-structured mapping. The ability to send and/or access complex data types as complete entities rather than lists of parameters promotes data abstraction and encapsulation, allowing greater flexibility through modular architecture as underlying data structures can change during the lifestyle or evolution of a computer based composition. Additionally, the facility to define data accessibility, and the ability to reuse human readable names based on a variable's scope is a common feature of most programming languages. This paradigm has been extended in that scoping a variable can be dynamically bound or addressed to specific objects, class types, devices or globally on an entire network. We describe the evolution of this paradigm through its development via various project requirements.},
  langid = {english},
  keywords = {Accelerometers,Australia,Internet of things,Music}
}

@inproceedings{goverChoirSingersPilot2021,
  title = {Choir {{Singers Pilot}} – {{An}} Online Platform for Choir Singers Practice},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Gover, Matan and Sarasúa, Álvaro and Parra, Hector and Janer, Jordi and Mayor, Oscar and Cuesta, Helena and Pascual, Maria Pilar and Gkiokas, Aggelos and Gomez, Emilia},
  editor = {Joglar-Ongay, Luis and Serra, Xavier and Font, Frederic and Tovstogan, Philip and Stolfi, Ariane and A. Correya, Albin and Ramires, Antonio and Bogdanov, Dmitry and Faraldo, Angel and Favory, Xavier},
  date = {2021-07},
  series = {Wac '21},
  publisher = {UPF},
  location = {Barcelona, Spain},
  issn = {2663-5844},
  abstract = {We present the Choir Singers Pilot, a web-based system that assists choir singers in their individual learning and practice. Our system is built using modern web technologies and provides singers with an interactive view of the musical score along with an aligned audio performance created using state-of-the-art singing synthesis technology. The Web Audio API is used to dynamically mix the choir voices and give users control over sound parameters. In-browser audio latency compensation is used to keep user recordings aligned to the reference music tracks. The pitch is automatically extracted from user recordings and can then be analyzed using an assessment algorithm to provide intonation ratings to the user. The system also facilitates communication and collaboration in choirs by enabling singers to share their recordings with conductors and receive feedback. Our work, in the larger scope of the TROMPA project, aims to enrich musical activities and promote use of digital music resources. To that end, we also synthesize thousands of public-domain choir scores and make them available in a searchable repository alongside relevant metadata for public consumption.}
}

@article{gresham-lancasterAestheticsHistoryHub1998,
  title = {The {{Aesthetics}} and {{History}} of the {{Hub}}: {{The Effects}} of {{Changing Technology}} on {{Network Computer Music}}},
  shorttitle = {The {{Aesthetics}} and {{History}} of the {{Hub}}},
  author = {Gresham-Lancaster, Scot},
  date = {1998},
  journaltitle = {Leonardo music journal},
  volume = {8},
  pages = {39--44},
  publisher = {MIT Press},
  issn = {0961-1215},
  doi = {10.2307/1513398},
  abstract = {The author, a member of the group the Hub, discusses the aesthetic and performance history of the group and related San Francisco Bay Area live interactive music performance practices. The performance practice of the Hub-interactive computer network music-is discussed. Particular focus is placed on the impact of changes in technology. Future applications and directions of this musical approach are discussed.},
  langid = {english},
  keywords = {Composers,Composition (Music),Computer software,Entertainers,Music,Musical instruments}
}

@article{harawaySituatedKnowledgesScience1988,
  title = {Situated {{Knowledges}}: {{The Science Question}} in {{Feminism}} and the {{Privilege}} of {{Partial Perspective}}},
  shorttitle = {Situated {{Knowledges}}},
  author = {Haraway, Donna},
  date = {1988},
  journaltitle = {Feminist Studies},
  volume = {14},
  number = {3},
  eprint = {3178066},
  eprinttype = {jstor},
  pages = {575--599},
  publisher = {Feminist Studies, Inc.},
  issn = {0046-3663},
  doi = {10.2307/3178066},
  url = {https://www.jstor.org/stable/3178066},
  urldate = {2024-08-15}
}

@book{heideggerBasicWritings2008,
  title = {Basic {{Writings}}},
  author = {Heidegger, Martin},
  translator = {Krell, David},
  date = {2008-11-04},
  edition = {Revised, Expanded ed. edition},
  publisher = {Harper Perennial},
  location = {New York},
  abstract = {"One of the most profound thinkers of the 20th century." -- New York TimesThe finest single-volume anthology of the great philosopher's work, with a new introduction by leading Heidegger scholar Taylor CarmanBasic Writings is the finest single-volume anthology of the work of Martin Heidegger, widely considered one of the most important modern philosophers. Its selections offer a full range of the influential author's writings--including "The Origin of the Work of Art," the introduction to Being and Time, "What Is Metaphysics?," "Letter on Humanism," "The Question Concerning Technology," "The Way to Language," and "The End of Philosophy." This essential collection provides readers with a concise introduction to the groundbreaking philosophy of this brilliant and essential thinker.},
  isbn = {978-0-06-162701-9},
  langid = {english},
  pagetotal = {480}
}

@incollection{heideggerQuestionConcerningTechnology2008,
  title = {The {{Question Concerning Technology}}},
  booktitle = {Basic {{Writings}}},
  author = {Heidegger, Martin},
  translator = {Krell, David},
  date = {2008-11-04},
  edition = {Revised, Expanded ed. edition},
  pages = {311--341},
  publisher = {Harper Perennial},
  location = {New York},
  isbn = {978-0-06-162701-9},
  langid = {english}
}

@book{hemptonOneSquareInch2010,
  title = {One {{Square Inch}} of {{Silence}}: {{One Man}}'s {{Quest}} to {{Preserve Quiet}}},
  shorttitle = {One {{Square Inch}} of {{Silence}}},
  author = {Hempton, Gordon},
  date = {2010-03-02},
  edition = {Illustrated edition},
  publisher = {Atria},
  location = {New York},
  isbn = {978-1-4165-5910-8},
  langid = {english},
  pagetotal = {368}
}

@inproceedings{hodlDesignImplicationsTechnologyMediated2017,
  title = {Design {{Implications}} for {{Technology-Mediated Audience Participation}} in {{Live Music}}},
  author = {Hödl, Oliver and Fitzpatrick, Geraldine and Kayali, Fares and Holland, Simon},
  date = {2017},
  pages = {28--34},
  url = {https://repositum.tuwien.at/handle/20.500.12708/57092},
  urldate = {2024-09-23},
  eventtitle = {Proceedings of {{Sound}} and {{Music Computing}} 2017},
  langid = {english},
  annotation = {Accepted: 2022-08-04T16:44:09Z}
}

@inproceedings{hollerwegerStreaaamFullyAutomated2021,
  title = {Streaaam: {{A}} Fully Automated Experimental Audio Streaming Server},
  shorttitle = {Streaaam},
  booktitle = {Proceedings of the 16th {{International Audio Mostly Conference}}},
  author = {Hollerweger, Florian},
  date = {2021},
  pages = {161--168},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3478384.3478426},
  abstract = {Streaaam is a fully automated experimental audio streaming server, built from Free/Libre Open Source Software in a higher education context. Some of its features include an automatic synthesized-speech moderator; real-time data integration via web APIs; and automatic loading and playing of generative music patches in Pd, SuperCollider, and Csound. These features are creatively combined in our stream’s program, such as in the form of a robot comedy show. Streaaam was conceived of and continues to be developed as a pedagogical vehicle in the context of an undergraduate education in audio arts, music technology, and sound design. Its goals are to inspire students to work creatively in the sonic arts, to introduce them to the development of audio applications for the web, and to showcase student art works at our department and college. This paper discusses the project’s technical implementation, our software development and program curation cycles, pedagogical experiences with the project thus far, and plans for future work.},
  isbn = {978-1-4503-8569-5},
  langid = {english}
}

@article{hopeSoundArtMobile2005,
  title = {Sound {{Art}} / {{Mobile Art}}},
  author = {Hope, Cat},
  date = {2005-01-01},
  journaltitle = {Sound Scripts},
  url = {https://ro.ecu.edu.au/csound/7}
}

@book{huiQuestionConcerningTechnology2016a,
  title = {The {{Question Concerning Technology}} in {{China}}: {{An Essay}} in {{Cosmotechnics}}},
  shorttitle = {The {{Question Concerning Technology}} in {{China}}},
  author = {Hui, Yuk},
  date = {2016-09-02},
  publisher = {Urbanomic},
  location = {Falmouth},
  abstract = {A systematic historical survey of Chinese thought is followed by an investigation of the historical-metaphysical questions of modern technology, asking how Chinese thought might contribute to a renewed questioning of globalized technics.Heidegger's critique of modern technology and its relation to metaphysics has been widely accepted in the East. Yet the conception that there is only one—originally Greek—type of technics has been an obstacle to any original critical thinking of technology in modern Chinese thought.Yuk Hui argues for the urgency of imagining a specifically Chinese philosophy of technology capable of responding to Heidegger's challenge, while problematizing the affirmation of technics and technologies as anthropologically universal.This investigation of the historical-metaphysical question of technology, drawing on Lyotard, Simondon, and Stiegler, and introducing a history of modern Eastern philosophical thinking largely unknown to Western readers, including philosophers such as Feng Youlan, Mou Zongsan, and Keiji Nishitani, sheds new light on the obscurity of the question of technology in China. Why was technics never thematized in Chinese thought? Why has time never been a real question for Chinese philosophy? How was the traditional concept of Qi transformed in its relation to Dao as China welcomed technological modernity and westernization?In The Question Concerning Technology in China, a systematic historical survey of the major concepts of traditional Chinese thinking is followed by a startlingly original investigation of these questions, in order to ask how Chinese thought might today contribute to a renewed, cosmotechnical questioning of globalized technics.},
  isbn = {978-0-9954550-0-9},
  langid = {english},
  pagetotal = {352}
}

@inproceedings{inkinDeclarativeWebAudio2021,
  title = {Declarative Web Audio {{API}}},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Inkin, Alexander},
  editor = {Joglar-Ongay, Luis and Serra, Xavier and Font, Frederic and Tovstogan, Philip and Stolfi, Ariane and A. Correya, Albin and Ramires, Antonio and Bogdanov, Dmitry and Faraldo, Angel and Favory, Xavier},
  date = {2021-07},
  series = {Wac '21},
  publisher = {UPF},
  location = {Barcelona, Spain},
  issn = {2663-5844},
  abstract = {In this talk, we will explore differences between imperative and declarative approach to Web Audio API. We will see what it takes to turn Web Audio API declarative with dependency injection pattern also known as inversion of control and what benefits it brings. We will create basic proof of concept using plain JavaScript and compare it to more advanced code based on Angular framework. All these techniques will help us write reusable structures and program music with Web Audio API. On top of that, we will look at Web MIDI API and create playable synthesizer using same declarative approach so we can perform music live. We will also touch on how using this knowledge we can create a Guitar Hero-like keys playing and Karaoke singing game which can be played directly in the browser, even on modern smartphones.}
}

@inproceedings{iorwerthApplicationNetworkedMusic2019,
  title = {The Application of {{Networked Music Performance}} Technology to Access Ensemble Activity for Socially Isolated Musicians},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Iorwerth, Miriam and Knox, Don},
  editor = {Xambó, Anna and Martín, Sara R. and Roma, Gerard},
  date = {2019-12},
  series = {Wac '19},
  pages = {8--13},
  publisher = {NTNU},
  location = {Trondheim, Norway},
  issn = {2663-5844},
  abstract = {Networked Music Performance (NMP) allows musicians to play together over distances via the internet. For musicians who are socially isolated this is a valuable tool to allow musical connections despite barriers of geography or mobility. There are, however, challenges when using this technology, including how musicians cope with technical challenges (such as latency, and setting up and using NMP software), but also the challenges of communicating via potentially degraded audio and video links. By examining current research and a case study of the use of NMP at the University of the Highlands and Islands in a remote part of the UK, this paper argues that these challenges are not insurmountable. Meaningful musical relationships can be built and maintained using typical domestic equipment, and the network environment gives opportunities for musical creativity that would not be possible in a conventional rehearsal space.}
}

@article{jiangLowLatencyNetworkingWhere2019,
  title = {Low-{{Latency Networking}}: {{Where Latency Lurks}} and {{How}} to {{Tame It}}},
  shorttitle = {Low-{{Latency Networking}}},
  author = {Jiang, Xiaolin and Shokri-Ghadikolaei, Hossein and Fodor, Gabor and Modiano, Eytan and Pang, Zhibo and Zorzi, Michele and Fischione, Carlo},
  date = {2019},
  journaltitle = {Proceedings of the IEEE},
  volume = {107},
  number = {2},
  pages = {280--306},
  publisher = {IEEE},
  location = {New York},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2018.2863960},
  abstract = {While the current generation of mobile and fixed communication networks has been standardized for mobile broadband services, the next generation is driven by the vision of the Internet of Things and mission-critical communication services requiring latency in the order of milliseconds or submilliseconds. However, these new stringent requirements have a large technical impact on the design of all layers of the communication protocol stack. The cross-layer interactions are complex due to the multiple design principles and technologies that contribute to the layers' design and fundamental performance limitations. We will be able to develop low-latency networks only if we address the problem of these complex interactions from the new point of view of submilliseconds latency. In this paper, we propose a holistic analysis and classification of the main design principles and enabling technologies that will make it possible to deploy low-latency wireless communication networks. We argue that these design principles and enabling technologies must be carefully orchestrated to meet the stringent requirements and to manage the inherent tradeoffs between low latency and traditional performance metrics. We also review currently ongoing standardization activities in prominent standards associations, and discuss open problems for future research.},
  langid = {english},
  keywords = {Communication,Computer networks,Internet of things,Mobile communication systems,Mobile computing,Reliability,Standardization,Wireless sensor networks}
}

@incollection{kanePierreSchaefferSound2014,
  title = {Pierre {{Schaeffer}}, the {{Sound Object}}, and the {{Acousmatic Reduction}}},
  booktitle = {Sound {{Unseen}}: {{Acousmatic Sound}} in {{Theory}} and {{Practice}}},
  author = {Kane, Brian},
  editor = {Kane, Brian},
  date = {2014-07-01},
  pages = {0},
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780199347841.003.0002},
  url = {https://doi.org/10.1093/acprof:oso/9780199347841.003.0002},
  urldate = {2024-09-27},
  abstract = {Pierre Schaeffer, the inventor of musique concrète, turned to phenomenology when attempting to ground a new field of research known as acousmatics (l’acousmatique). This chapter traces the historical development of Schaeffer’s “sound object” (l’objet sonore) and its relation to the acousmatic reduction. By modelling his research after Husserl’s phenomenology, Schaeffer developed a method for bracketing sounds from their sources and causes, the acousmatic reduction, akin to Husserl’s epoché. The acousmatic reduction permits the differentiation of four primary modes of listening. “Reduced listening” (l’écoute réduite) is identified as the mode appropriate for auditioning the sound object. A sound object is an intentional object whose essential properties can be disclosed through a method of imaginative variation. The connection between Schaeffer’s theory and his later compositions is also discussed. After a thorough exposition of Schaeffer’s phenomenology, three objections to his theory are presented concerning myth, phantasmagoria, and ontology.},
  isbn = {978-0-19-934784-1}
}

@inproceedings{kayaliPlayfulTechnologyMediatedAudience2017,
  title = {Playful {{Technology-Mediated Audience Participation}} in a {{Live Music Event}}},
  booktitle = {Extended {{Abstracts Publication}} of the {{Annual Symposium}} on {{Computer-Human Interaction}} in {{Play}}},
  author = {Kayali, Fares and Hödl, Oliver and Fitzpatrick, Geraldine and Purgathofer, Peter and Filipp, Alexander and Mateus-Berr, Ruth and Kühn, Ulrich and Wagensommerer, Thomas and Kretz, Johannes and Kirchmayr, Susanne},
  date = {2017-10-15},
  series = {{{CHI PLAY}} '17 {{Extended Abstracts}}},
  pages = {437--443},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3130859.3131293},
  url = {https://dl.acm.org/doi/10.1145/3130859.3131293},
  urldate = {2024-09-22},
  abstract = {This paper presents the evaluation of playful technology-mediated audience participation (TMAP) during three music performances in a recent music event. It captures preliminary impressions from a wide range of perspectives and includes critical reflections of music artists, video analysis and qualitative interviews with audience members to cover hypotheses designed to capture both the artists' and the audience's point of view. Results indicate a willingness from both sides to engage in playful TMAP, and a high potential for exploration and playful collaboration within the audience, but the experience is restricted by the need to retain control on the side of artists and the need for clear instructions, feedback and reliable technical systems on the side of the audience.},
  isbn = {978-1-4503-5111-9}
}

@article{kellerUbimusContributionsDigital2024,
  title = {Ubimus Contributions to Digital Creative Practices ({{Editorial}})},
  author = {Keller, Damián and Lazzarini, Victor and Turchet, Luca and Brooks, Anthony L.},
  date = {2024},
  journaltitle = {Digital creativity (Exeter)},
  volume = {35},
  number = {1},
  pages = {1--12},
  publisher = {Routledge},
  issn = {1462-6268},
  doi = {10.1080/14626268.2024.2334027},
  langid = {english}
}

@incollection{kellerUbimusLensCreativity2014,
  title = {Ubimus {{Through}} the {{Lens}} of {{Creativity Theories}}},
  booktitle = {Ubiquitous {{Music}}},
  author = {Keller, Damián and Lazzarini, Victor and Pimenta, Marcelo S.},
  editor = {Keller, Damián and Lazzarini, Victor and Pimenta, Marcelo S.},
  date = {2014},
  pages = {3--23},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-11152-0_1},
  url = {https://doi.org/10.1007/978-3-319-11152-0_1},
  urldate = {2024-09-20},
  abstract = {We place Ubiquitous Music research within the context of current creativity theories. Given the ongoing theoretical discussion on the relevance of domain-specific or general approaches to creativity, this chapter covers general creativity frameworks and music-oriented models. First, we introduce the creativity factors, magnitudes and constructs that are applicable to UbiMus research. Then we discuss the limitations of several models of creativity that have been laid out within the context of music education and music psychology research. The analysis points to two caveats: lack of material grounding and no support for the early stages of creative activity. Key concepts are proposed to handle ecologically grounded creative practice and everyday musical phenomena.},
  isbn = {978-3-319-11152-0},
  langid = {english},
  keywords = {Compositional Process,Creative Activity,General Creativity,Material Resource,Musical Activity}
}

@book{kellerUbiquitousMusic2014,
  title = {Ubiquitous Music},
  editor = {Keller, Damián and Lazzarini, Victor and Pimenta, Marcelo S.},
  date = {2014},
  series = {Computational Music Science},
  publisher = {Springer},
  location = {Cham},
  abstract = {This is the first monograph dedicated to this interdisciplinary research area, combining the views of music, computer science, education, creativity studies, psychology, and engineering. The contributions include introductions to ubiquitous music research, featuring theory, applications, and technological development, and descriptions of permanent community initiatives such as virtual forums, multi-institutional research projects, and collaborative publications. The book will be of value to researchers and educators in all domains engaged with creativity, computing, music, and digital arts},
  isbn = {978-3-319-11151-3},
  langid = {english},
  pagetotal = {153},
  keywords = {CSound (Computer program language),Csound (Langage de programmation),Informatique omniprésente,Music and technology,Music Computer network resources,Music Computer programs,Musique et technologie,Musique Logiciels,Ubiquitous computing},
  annotation = {OCLC: 889163307}
}

@inproceedings{kleimolaNativeWebAudio2018,
  title = {Native Web Audio {{API}} Plugins},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Kleimola, Jari and Campbell, Owen},
  editor = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
  date = {2018-09},
  series = {Wac '18},
  publisher = {TU Berlin},
  location = {Berlin, Germany},
  issn = {2663-5844},
  abstract = {This work enables native audio plugin development using the Web Audio API and other web technologies. Hybrid forms where DSP algorithms are implemented in both JavaScript and native C++, and distributed forms where web technologies are used only for the user interface, are also supported. Various implementation options are explored, and the most promising option is implemented and evaluated. We found that the solution is able to operate at 128 sample buffer sizes, and that the performance of the Web Audio API audio graph is not compromised. The proof-of-concept solution also maintains compatibility with existing Web Audio API implementations. The average MIDI latency was 24 ms, which is high when comparing with fully native plugin solutions. Backwards compatibility also reduces usability when working with multiple plugin instances. We conclude that the second iteration needs to break backwards compatibility in order to overcome the MIDI latency and multi-plugin support issues.}
}

@inproceedings{kritsisIMuSciCAWebPlatform2019,
  title = {{{iMuSciCA}}: A Web Platform for Science Education through Music Activities},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Kritsis, Kosmas and Bouillon, Manuel and Martín-Albo, Daniel and Acosta, Carlos and Piechaud, Robert and Katsouros, Vassilis},
  editor = {Xambó, Anna and Martín, Sara R. and Roma, Gerard},
  date = {2019-12},
  series = {Wac '19},
  pages = {35--40},
  publisher = {NTNU},
  location = {Trondheim, Norway},
  issn = {2663-5844},
  abstract = {In this paper we present the iMuSciCA web platform which addresses secondary school students with the aim to support mastery of core academic content on STEM subjects (Physics, Geometry, Mathematics,and Technology) alongside with the development of creativity and deeper learning skills through the students' engagement in music activities. Herein we focus on the technical implementation of the various music related tools and Activity Environments hosted by the iMuSciCA workbench, which are exclusively developed with modern web technologies.}
}

@book{lacanOtherSidePsychoanalysis2007,
  title = {The {{Other Side}} of {{Psychoanalysis}}: {{The Seminar}} of {{Jacques Lacan}}, {{Book XVII}}},
  shorttitle = {The {{Other Side}} of {{Psychoanalysis}}},
  author = {Lacan, Jacques},
  translator = {Grigg, Russell},
  date = {2007},
  edition = {1st edition},
  publisher = {W. W. Norton \& Company, Inc.},
  location = {New York (N. Y.)},
  abstract = {This new translation of Jacques Lacan's deliberation on psychoanalysis and contemporary social order offers welcome, readable access to the brilliant author's seminal thinking on Freud, Marx, and Hegel; patterns of social and sexual behavior; and the nature and function of science and knowledge in the Australian contemporary world.},
  isbn = {978-0-393-33040-3},
  langid = {english},
  pagetotal = {224}
}

@inproceedings{lakiotakisApplicationnetworkCollaborationUsing2017,
  title = {Application-Network Collaboration Using {{SDN}} for Ultra-Low Delay Teleorchestras},
  booktitle = {2017 {{IEEE Symposium}} on {{Computers}} and {{Communications}} ({{ISCC}})},
  author = {Lakiotakis, Emmanouil and Liaskos, Christos and Dimitropoulos, Xenofontas},
  date = {2017-07},
  pages = {70--75},
  doi = {10.1109/ISCC.2017.8024507},
  url = {https://ieeexplore.ieee.org/document/8024507},
  urldate = {2024-09-22},
  abstract = {Networked Music Performance (NMP) constitutes a class of ultra-low delay sensitive applications, allowing geographically separate musicians to perform seamlessly as a tele-orchestra. For this application type, the QoS indicator is the mouth-to-ear delay, which should be kept under 25 milliseconds. The mouth-to-ear delay comprises signal processing latency and network delay. We propose a strong collaboration between the network and NMP applications to actively keep the to mouth-to-ear delay minimal, using direct state notifications. Related approaches can be characterized as passive, since they try to estimate the network state indirectly, based on the end application performance. Our solution employs Software Defined Networking (SDN) to implement the network-to-application collaboration, being facilitated by the well-defined network interface that SDN offers. Emulation results show that the proposed scheme achieves an improvement of up to 59\% in mouth-to-ear delay over the existing passive solutions.},
  eventtitle = {2017 {{IEEE Symposium}} on {{Computers}} and {{Communications}} ({{ISCC}})},
  keywords = {Delays,Internet,Mathematical model,Monitoring,Networked Music Performance,Quality of service,Quality of Service,Receivers,Software Defined Networking,Transmitters,ultra-low delay sensitive}
}

@inproceedings{lanGlicolGraphorientedLive2021,
  title = {Glicol: A Graph-Oriented Live Coding Language Developed with Rust, {{WebAssembly}} and {{AudioWorklet}}},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Lan, Qichao and Jensenius, Alexander Refsum},
  editor = {Joglar-Ongay, Luis and Serra, Xavier and Font, Frederic and Tovstogan, Philip and Stolfi, Ariane and A. Correya, Albin and Ramires, Antonio and Bogdanov, Dmitry and Faraldo, Angel and Favory, Xavier},
  date = {2021-07},
  series = {Wac '21},
  publisher = {UPF},
  location = {Barcelona, Spain},
  issn = {2663-5844},
  abstract = {This paper introduces the new music live coding language Glicol (graph-oriented live coding language) and its web-based run-time environment. As the name suggests, this language is designed to represent directed acyclic graphs (DAG), using a syntax optimised for live music performances. The audio engine and the language interpreter are both developed with the Rust programming language. With the help of WebAssembly and AudioWorklet, this language can run in web browsers. It also enables co-performance with the support for collaborative editing. Taking advantages of the Rust programming language design, the run-time environment is both safe and efficient. Documentation and error handling messages can be accessed in the web browser. All in all, we see Glicol as an efficient and future-oriented language for collaborative text-based musicking.}
}

@incollection{lazzariniDevelopmentToolsUbiquitous2014,
  title = {Development {{Tools}} for {{Ubiquitous Music}} on the {{World Wide Web}}},
  booktitle = {Ubiquitous {{Music}}},
  author = {Lazzarini, Victor and Costello, Edward and Yi, Steven and {ffitch}, John},
  editor = {Keller, Damián and Lazzarini, Victor and Pimenta, Marcelo S.},
  date = {2014},
  pages = {111--128},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-11152-0_6},
  url = {https://doi.org/10.1007/978-3-319-11152-0_6},
  urldate = {2024-09-20},
  abstract = {This chapter discusses two approaches to provide a general-purpose audio programming support for Ubiquitous Music web applications. It reviews the current state of web audio development and discusses some previous attempts at this. We then introduce a JavaScript version of Csound that has been created using the Emscripten compiler and discuss its features and limitations. In complement to this, we look at a Native Client implementation of Csound, which is a fully functional version of Csound running in Chrome and Chromium browsers.},
  isbn = {978-3-319-11152-0},
  langid = {english},
  keywords = {Audio Input,Audio Processing,Control Channel,Main Thread,Output Buffer}
}

@book{lazzariniUbiquitousMusicEcologies2020,
  title = {Ubiquitous {{Music Ecologies}}},
  author = {Lazzarini, Victor and Keller, Damián and Otero, Nuno and Turchet, Luca},
  date = {2020},
  publisher = {Taylor \& Francis Group},
  location = {Oxford, UNITED KINGDOM},
  url = {http://ebookcentral.proquest.com/lib/rmit/detail.action?docID=6371513},
  urldate = {2024-09-19},
  isbn = {978-1-00-025860-8},
  keywords = {Music-Data processing}
}

@incollection{lazzariniUbiquitousMusicEcosystems2014,
  title = {Ubiquitous {{Music Ecosystems}}: {{Faust Programs}} in {{Csound}}},
  shorttitle = {Ubiquitous {{Music Ecosystems}}},
  booktitle = {Ubiquitous {{Music}}},
  author = {Lazzarini, Victor and Keller, Damián and Pimenta, Marcelo and Timoney, Joseph},
  editor = {Keller, Damián and Lazzarini, Victor and Pimenta, Marcelo S.},
  date = {2014},
  pages = {129--150},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-11152-0_7},
  url = {https://doi.org/10.1007/978-3-319-11152-0_7},
  urldate = {2024-09-20},
  abstract = {This chapter describes the combination of two high-level audio and music programming systems, Faust and Csound. The latter is a MUSIC N-derived language, with a large set of unit generators and a long history of development. The former is a purely functional language designed to describe audio processing algorithms that can be compiled into a variety of formats. The two systems are combined in the Faust Csound opcodes, which allow the on-the-fly programming, compilation and instantiation of Faust DSP programs in a running Csound environment. Examples are presented, and the concept of Ubiquitous Music Ecosystem is discussed.},
  isbn = {978-3-319-11152-0},
  langid = {english},
  keywords = {Application Programming Interface,Computer Music,Digital Signal Processing,Signal Processing Algorithm,Virtual Void}
}

@online{lemayFlamingLips20,
  title = {The {{Flaming Lips}}: 20 {{Years}} of {{Weird}}: {{Flaming Lips}} 1986-2006},
  shorttitle = {The {{Flaming Lips}}},
  author = {LeMay, Matt},
  url = {https://pitchfork.com/reviews/albums/3088-20-years-of-weird-flaming-lips-1986-2006/},
  urldate = {2024-09-28},
  abstract = {Odds and sods live collection issued to coincide with the release of the Oklahoma band's Fearless Freaks film, the record also features a studio rarity and, uh, "Free Radicals".},
  langid = {american},
  organization = {Pitchfork}
}

@inproceedings{letzFAUSTOnlineIDE2019,
  title = {{{FAUST}} Online {{IDE}}: Dynamically Compile and Publish {{FAUST}} Code as {{WebAudio Plugins}}},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Letz, Stéphane and Ren, Shihong and Orlarey, Yann and Michon, Romain and Fober, Dominique and Aamari, ElMehdi and Buffa, Michel and Lebrun, Jerome},
  editor = {Xambó, Anna and Martín, Sara R. and Roma, Gerard},
  date = {2019-12},
  series = {Wac '19},
  pages = {71--76},
  publisher = {NTNU},
  location = {Trondheim, Norway},
  issn = {2663-5844},
  abstract = {Developing or porting existing audio and MIDI effects or instruments for the Web platform is a hot topic. Several initiatives, like business enterprise based ones (Propellerhead recently presented its Rack Extension running on the Web1), to more community based open-source projects are emerging. All of them aim to facilitate porting existing code base (usually developed in native languages like C/C++) as well as facilitating the use of existing audio DSP languages and platforms.We present a solution based around the FAUST DSP audio language, its redesigned Web based editor, and the integration of a plugin GUI editor allowing to directly test, generate and deploy WebAudio Plugins (WAP). This recently proposed plugin format aims to facilitate the integration of pure native WebAudio based components, some ported from native languages like C/C++, as well as plugins written in audio DSP Domain Specific Languages. The paper will describe the complete workflow, from the Faust DSP source written and tested in a fully functional editor to a self-contained plugin running in a separate host application.}
}

@online{levinDialtonesTelesymphony2020,
  title = {Dialtones ({{A Telesymphony}})},
  author = {Levin, Golan},
  date = {2020},
  url = {https://www.flong.com/archive/projects/telesymphony/index.html},
  urldate = {2024-09-28}
}

@online{levinInformalCatalogueMobile2020,
  title = {An {{Informal Catalogue}} of {{Mobile Phone Performances}}, {{Installations}} and {{Artworks}}, 2001-2004},
  author = {Levin, Golan},
  date = {2020},
  url = {https://www.flong.com/archive/texts/lists/mobile_phone/index.html},
  urldate = {2024-09-28}
}

@inproceedings{lindChallengesDevelopmentEasyaccess2021,
  title = {Challenges in the Development of an Easy-Access Mobile Phone Orchestra Platform},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Lind, Anders and Yttergren, Björn and Gustafson, Håkan},
  editor = {Joglar-Ongay, Luis and Serra, Xavier and Font, Frederic and Tovstogan, Philip and Stolfi, Ariane and A. Correya, Albin and Ramires, Antonio and Bogdanov, Dmitry and Faraldo, Angel and Favory, Xavier},
  date = {2021-07},
  series = {Wac '21},
  publisher = {UPF},
  location = {Barcelona, Spain},
  issn = {2663-5844},
  abstract = {How can an easy-access Mobile Phone Orchestra (MPO) platform be developed to facilitate high-end, flexible artistic expressions suitable for concert hall performances? Moreover, what challenges and possibilities arise in the development process of a novel MPO platform embracing school children as performers? In this paper, we discuss potential answers to these questions, which were integrated in the development process of mobilephoneorchestra.com. In particular, we highlight the main challenge of developing a performance platform that is both mobile and artistically rewarding, but still easily accessed by schoolchildren. Mobilephoneorchestra.com was developed as a Web Audio API to enable large-scale concert hall performances of fixed polyphonic contemporary art music. It is a kind of philharmonic orchestra for electronic sounds that embraces school children as performers and uses smart phones as instruments. Through an autoethnographic approach, our research was carried out in multiple iterations, inspired by action research. In this paper, we present findings from three iterations that include performances of music involving MPO. Schoolchildren/youths between 10 and 17 years old were addressed as MPO performers. The findings reveal both the artistic challenges of the platform and the possibilities. We specifically highlight the use of mobile music interfaces in combination with animated notation as a novel approach for an MPO concept.}
}

@inproceedings{lindetorpPuttingWebAudio2021,
  title = {Putting Web Audio {{API}} to the Test: {{Introducing WebAudioXML}} as a Pedagogical Platform},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Lindetorp, Hans and Falkenberg, Kjetil},
  editor = {Joglar-Ongay, Luis and Serra, Xavier and Font, Frederic and Tovstogan, Philip and Stolfi, Ariane and A. Correya, Albin and Ramires, Antonio and Bogdanov, Dmitry and Faraldo, Angel and Favory, Xavier},
  date = {2021-07},
  series = {Wac '21},
  publisher = {UPF},
  location = {Barcelona, Spain},
  issn = {2663-5844},
  abstract = {Web technologies in general and Web Audio API in particular have a great potential as a learning platform for developing interactive sound and music applications. Earlier studies at the Royal College of Music in Stockholm have led to a wide range of student projects but have also indicated that there is a high threshold for novice programmers to understand and use Web Audio API. We developed the WebAudioXML coding environment to solve this problem, and added a statistics module to analyze student works. Three groups of students with technical respectively artistic background participated through online courses by building interactive, sound-based applications. We analysed the projects to understand the impact WebAudioXML has on creativity and the learning process. The results indicate that WebAudioXML can be a useful platform for teaching and learning how to build online audio applications. The platform makes mapping between user interactions and audio parameters accessible for novice programmer and supports artists in successfully realizing their design ideas. We show that templates can be a great help for the students to get started but also a limitation for them to expand ideas beyond the presented scope.}
}

@book{lovelessHowMakeArt2019,
  title = {How to Make Art at the End of the World: A Manifesto for Research-Creation},
  shorttitle = {How to Make Art at the End of the World},
  author = {Loveless, Natalie},
  date = {2019},
  publisher = {Duke University Press},
  location = {Durham},
  abstract = {In recent years, the rise of research-creation - a scholarly activity that considers art practices as research methods in their own right - has emerged from the organic convergences of the arts and interdisciplinary humanities, and it has been fostered by universities wishing to enhance their public profiles. In 'How to Make Art at the End of the World' Natalie Loveless draws on diverse perspectives-from feminist science studies to psychoanalytic theory, as well as her own experience advising undergraduate and graduate students-to argue for research-creation as both a means to produce innovative scholarship and a way to transform pedagogy and research within the contemporary neoliberal university. Championing experimental, artistically driven methods of teaching, researching, and publication, research-creation works to render daily life in the academy more pedagogically, politically, and affectively sustainable, as well as more responsive to issues of social and ecological justice.},
  isbn = {978-1-4780-0402-8},
  langid = {english},
  keywords = {Aims and objectives,Arts,Creation (Literary artistic etc.),Education Higher,Moral and ethical aspects,Research,Social aspects,Study and teaching (Higher)}
}

@inproceedings{manassehPlayPlaceExperiencing2021,
  title = {Play the Place: {{Experiencing}} Architectural Spaces Using a Web-Based {{3D}} Audio Environment},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Manasseh, Clifford M and Barthet, Mathieu and Zimmerli, Stephan},
  editor = {Joglar-Ongay, Luis and Serra, Xavier and Font, Frederic and Tovstogan, Philip and Stolfi, Ariane and A. Correya, Albin and Ramires, Antonio and Bogdanov, Dmitry and Faraldo, Angel and Favory, Xavier},
  date = {2021-07},
  series = {Wac '21},
  publisher = {UPF},
  location = {Barcelona, Spain},
  issn = {2663-5844},
  abstract = {Assessing the acoustics of an architectural space before its physical construction is of interest to architects and structural engineers. In this work, we present a browser-based interactive app enabling sonic interaction in a virtual environment modeled after a small studio architectural design for music ideation and creation made by architects Stephan \& Eric Zimmerli, called ‘Studiolo’. We first describe our ideation and design process involving pilot testing in the New Atlantis online environment for a collaborative sound and music experience. We then describe the implementation of the ‘Studiolo’ web app prototype supported by Javascript packages (Tone, Resonance Audio, Three, and Tween) and the Web Audio API. Room acoustics is modeled using the EVERTims framework providing a real-time auralization engine for binaural room impulse response approximation. A critical analysis by one of the architects highlights the importance of multimodal factors, from visual mapping textures and lighting quality to more sensitive and responsive timbres addressing the “tactile dimension” of music creation.}
}

@book{mattinSocialDissonance2022,
  title = {Social {{Dissonance}}},
  author = {Mattin},
  date = {2022-05-24},
  publisher = {MIT Press},
  location = {Falmouth},
  abstract = {An argument that by amplifying alienation in performance, we can shift the emphasis from the sonic to the socialAn argument that by amplifying alienation in performance, we can shift the emphasis from the sonic to the social.Works in sound studies continue to seek out sound "itself"-but, today, when the aesthetic can claim no autonomy and the agency of both artist and audience is socially constituted, why not explore the social mediation already present within our experience of the sonorous? In this work, artist, musician, performer, and theorist Mattin sets out an understanding of alienation as a constitutive part of subjectivity and as an enabling condition for exploring social dissonance-the discrepancy between our individual narcissism and our social capacity.Mattin's theoretical investigation is intertwined with documentation of a concrete experiment in the form of an instructional score (performed at documenta 14, 2017, in Athens and Kassel) which explores these conceptual connotations in practice, as players use members of the audience as instruments, who then hear themselves and reflect on their own conception and self-presentation. Social Dissonance claims that, by amplifying alienation in performance and participation in order to understand how we are constructed through various forms of mediation, we can shift the emphasis from the sonic to the social, and in doing so, discover for ourselves that social dissonance is the territory within which we already find ourselves, the condition we inhabit.},
  isbn = {978-1-913029-81-4},
  langid = {english},
  pagetotal = {256}
}

@inproceedings{matuszewskiWebAudioThings2018,
  title = {Toward a {{Web}} of {{Audio Things}}},
  booktitle = {Sound and {{Music Computing Conference}}},
  author = {Matuszewski, Benjamin and Bevilacqua, Frédéric},
  date = {2018-07},
  location = {Limassol, Cyprus},
  url = {https://hal.science/hal-01874968},
  urldate = {2024-09-19},
  abstract = {Recent developments of web standards, such as WebAu-dio, WebSockets or WebGL, has permitted new potentialities and developments in the field of interactive music systems. Until now, research and development efforts have principally focused on the exploration and validation of the concepts and on building prototypes. Nevertheless, it remains important to provide stable and powerful development environments for artists or researchers. The present paper aims at proposing foundations to the development of an experimental system, by analysing salient properties of existing computer music systems, and showing how these properties could be transposed to web-based distributed systems. Particularly, we argue that changing our perspective from a Mobile Web to a Web of Thing approach could allow us to tackle recurrent problems of web-based setups. We finally describe a first implementation of the proposed platform and two prototype applications.}
}

@article{matuszewskiWebBasedFrameworkDistributed2020,
  title = {A {{Web-Based Framework}} for {{Distributed Music System Research}} and {{Creation}}},
  author = {Matuszewski, Benjamin},
  date = {2020-10},
  journaltitle = {AES - Journal of the Audio Engineering Society Audio-Accoustics-Application},
  publisher = {Audio Engineering Society Inc},
  url = {https://hal.science/hal-03033143},
  urldate = {2024-09-19},
  abstract = {This paper presents soundworks, a framework dedicated to prototyping and developing distributed multimedia applications using Web technologies. Since its first release in 2015, the framework has been used in numerous artistic and research projects such as concerts, installations, workshops, teaching or experimental setups. We first present how this diversity of contexts and objectives permitted to identify a set of patterns able to support recurring needs of expert users in exploratory tasks. We then detail new developments that have been achieved to provide better support to these patterns. More particularly, we describe the novel distributed state management system dedicated at simplifying the implementation of remote control and monitoring interfaces and, the plug-in system implemented to improve the extensibility of the framework and foster composition of dedicated functionalities. We believe that these new developments can provide a solid ground to further research and artistic practices in the area of distributed music systems. The soundworks framework is open-source and released under BSD-3-Clause license.}
}

@book{mcluhanUnderstandingMediaExtensions1994a,
  title = {Understanding {{Media}}: {{The Extensions}} of {{Man}}},
  shorttitle = {Understanding {{Media}}},
  author = {McLuhan, Marshall and Lapham, Lewis H.},
  date = {1994-10-20},
  edition = {Reprint edition},
  publisher = {The MIT Press},
  location = {Cambridge, Mass.},
  abstract = {Terms and phrases such as "the global village" and "the medium is the message" are now part of the lexicon, and McLuhan's theories continue to challenge our sensibilities and our assumptions about how and what we communicate.This reissue of Understanding Media marks the thirtieth anniversary (1964-1994) of Marshall McLuhan's classic expose on the state of the then emerging phenomenon of mass media. Terms and phrases such as "the global village" and "the medium is the message" are now part of the lexicon, and McLuhan's theories continue to challenge our sensibilities and our assumptions about how and what we communicate.There has been a notable resurgence of interest in McLuhan's work in the last few years, fueled by the recent and continuing conjunctions between the cable companies and the regional phone companies, the appearance of magazines such as WiRed, and the development of new media models and information ecologies, many of which were spawned from MIT's Media Lab. In effect, media now begs to be redefined. In a new introduction to this edition of Understanding Media, Harper's editor Lewis Lapham reevaluates McLuhan's work in the light of the technological as well as the political and social changes that have occurred in the last part of this century.},
  isbn = {978-0-262-63159-4},
  langid = {english},
  pagetotal = {389}
}

@inproceedings{mitchellMakingMostWiFi2014,
  title = {Making the {{Most}} of {{Wi-Fi}}: {{Optimisations}} for {{Robust Wireless Live Music Performance}}},
  shorttitle = {Making the {{Most}} of {{Wi-Fi}}},
  author = {Mitchell, Thomas and Madgwick, Sebastian and Rankine, Simon and Hilton, Geoffrey and Freed, Adrian and Nix, Andrew},
  date = {2014},
  pages = {251--256},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1178875},
  url = {https://nime.org/proc/tmitchell2014/index.html},
  urldate = {2024-09-22},
  eventtitle = {Proceedings of the {{International Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  langid = {american}
}

@article{ondaEverywhereOnceJose2019,
  title = {Everywhere at {{Once}}: {{José Maceda}}'s {{Musical Territory}}},
  shorttitle = {Everywhere at {{Once}}},
  author = {Onda, Aki},
  date = {2019},
  journaltitle = {BOMB},
  number = {147},
  eprint = {26876309},
  eprinttype = {jstor},
  pages = {145--151},
  publisher = {New Art Publications},
  issn = {0743-3204},
  url = {https://www.jstor.org/stable/26876309},
  urldate = {2024-09-25}
}

@incollection{pimentaMethodsCreativityCentredDesign2014,
  title = {Methods in {{Creativity-Centred Design}} for {{Ubiquitous Musical Activities}}},
  booktitle = {Ubiquitous {{Music}}},
  author = {Pimenta, Marcelo S. and Keller, Damián and Flores, Luciano V. and family=Lima, given=Maria Helena, prefix=de, useprefix=true and Lazzarini, Victor},
  editor = {Keller, Damián and Lazzarini, Victor and Pimenta, Marcelo S.},
  date = {2014},
  pages = {25--48},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-11152-0_2},
  url = {https://doi.org/10.1007/978-3-319-11152-0_2},
  urldate = {2024-09-20},
  abstract = {In this chapter we describe a set of creativity-centred design methods including strategies for interaction, signal processing, planning, prototyping and creativity assessment. Social, material and procedural requirements were gathered through a ten-subject planning design study. Based on these results, an interaction metaphor—time tagging—was developed to deal with a musical activity in ubiquitous contexts: localised audio mixing. We implemented a series of prototypes—the first generation of mixDroid—for mixing using Android-based mobile devices. An exploratory field study with mixDroid was conducted inside the studio and in the locations where the sound samples were recorded. Activities happening outside the studio resulted in higher creativity scores on two dimensions—explorability and productivity. We discuss the preliminary implications of these findings for future experiments targeting aspects of exploratory creativity in everyday settings.},
  isbn = {978-3-319-11152-0},
  langid = {english},
  keywords = {Creative Activity,Musical Activity,Musical Experience,Musical Training,Sound Sample}
}

@article{renaudNetworkedMusicPerformance2012,
  title = {Networked {{Music Performance}}: {{State}} of the {{Art}}},
  shorttitle = {Networked {{Music Performance}}},
  author = {Renaud, Alain and Carôt, Alexander and Rebelo, Pedro},
  date = {2012-01-01}
}

@inproceedings{renBuildWebAudioJavaScript2021,
  title = {Build {{WebAudio}} and {{JavaScript}} Web Applications Using Jspatcher: A Web-Based Visual Programming Editor},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Ren, Shihong and Pottier, Laurent and Buffa, Michel},
  editor = {Joglar-Ongay, Luis and Serra, Xavier and Font, Frederic and Tovstogan, Philip and Stolfi, Ariane and A. Correya, Albin and Ramires, Antonio and Bogdanov, Dmitry and Faraldo, Angel and Favory, Xavier},
  date = {2021-07},
  series = {Wac '21},
  publisher = {UPF},
  location = {Barcelona, Spain},
  issn = {2663-5844},
  abstract = {Many visual programming languages (VPLs) such as Max [1] or PureData [2] provide a graphic canvas to allow developers to connect functions or data between them. This canvas, also known as a patcher [3], is basically a graph meant to be interpreted as dataflow computation by the system. Some VPLs are used for multimedia performance or content generation as the UI system is often an important part of the language. This paper presents a web-based VPL, JSPatcher, which allows not only to build audio graphs using the WebAudio API, but also to design graphically AudioWorklet DSPs with FAUST toolchain, [4] [5] or to create interactive programs with other language built-ins, Web APIs or any JavaScript modules.}
}

@inproceedings{robertsBringingTidalCyclesMinilanguage2019,
  title = {Bringing the {{TidalCycles}} Mini-Language to the Web},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Roberts, Charlie and Pachón-Puentes, Mariana},
  editor = {Xambó, Anna and Martín, Sara R. and Roma, Gerard},
  date = {2019-12},
  series = {Wac '19},
  pages = {98--102},
  publisher = {NTNU},
  location = {Trondheim, Norway},
  issn = {2663-5844},
  abstract = {TidalCycles has rapidly become the most popular system for many styles of live coding performance, in particular Algoraves. We created a JavaScript dialect of its mini-notation for pattern, enabling easy integration with creative coding tools. Our research pairs a formalism describing the mini-notation with a small JavaScript library for generating events over time; this library is suitable for generating events inside of an AudioWorkletProcessor thread and for assisting with scheduling in JavaScript environments more generally. We describe integrating the library into the two live coding systems, Gibber and Hydra, and discuss an accompanying technique for visually annotating the playback of TidalCycles patterns over time.}
}

@article{rottondiOverviewNetworkedMusic2016,
  title = {An {{Overview}} on {{Networked Music Performance Technologies}}},
  author = {Rottondi, Cristina and Chafe, Chris and Allocchio, Claudio and Sarti, Augusto},
  date = {2016},
  journaltitle = {IEEE Access},
  volume = {4},
  pages = {8823--8843},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2016.2628440},
  url = {https://ieeexplore.ieee.org/document/7769205},
  urldate = {2024-09-22},
  abstract = {Networked music performance (NMP) is a potential game changer among Internet applications, as it aims at revolutionizing the traditional concept of musical interaction by enabling remote musicians to interact and perform together through a telecommunication network. Ensuring realistic performance conditions, however, constitutes a significant engineering challenge due to the extremely strict requirements in terms of network delay and audio quality, which are needed to maintain a stable tempo, a satisfying synchronicity between performers and, more generally, a high-quality interaction experience. In this paper, we offer a review of the psycho-perceptual studies conducted in the past decade, aimed at identifying latency tolerance thresholds for synchronous real-time musical performance. We also provide an overview of hardware/software enabling technologies for NMP, with a particular emphasis on system architecture paradigms, networking configurations, and applications to real use cases.},
  eventtitle = {{{IEEE Access}}},
  keywords = {audio systems,Audio systems,Audio visual systems,audio-visual systems,Computer generated music,Internet,Music,network latency,networked music performance,Performance evaluatoin}
}

@inproceedings{sacchettoJackTripWebRTCNetworkedMusic2021,
  title = {{{JackTrip-WebRTC}}: {{Networked}} Music Experiments with {{PCM}} Stereo Audio in a {{Web}} Browser},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Sacchetto, Matteo and Servetti, Antonio and Chafe, Chris},
  editor = {Joglar-Ongay, Luis and Serra, Xavier and Font, Frederic and Tovstogan, Philip and Stolfi, Ariane and A. Correya, Albin and Ramires, Antonio and Bogdanov, Dmitry and Faraldo, Angel and Favory, Xavier},
  date = {2021-07},
  series = {Wac '21},
  publisher = {UPF},
  location = {Barcelona, Spain},
  issn = {2663-5844},
  abstract = {A large number of web applications are available for videoconferencing and those have been very helpful during the lockdown periods caused by the COVID-19 pandemic. However, none of these offer high fidelity stereo audio for music performance, mainly because the current WebRTC RTCPeerConnection standard only supports compressed audio formats. This paper presents the results achieved implementing 16-bit PCM stereo audio transmission on top of the WebRTC RTCDataChannel with the help of Web Audio and AudioWorklets. Several measurements with different configurations, browsers, and operating systems are presented. They show that, at least on the loopback network interface, this approach can achieve better quality and lower latency than using RTCPeerConnection, i.e., latencies as low as 50-60 ms have been achieved on MacOS.}
}

@article{shiEdgeComputingVision2016,
  title = {Edge {{Computing}}: {{Vision}} and {{Challenges}}},
  shorttitle = {Edge {{Computing}}},
  author = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
  date = {2016-10},
  journaltitle = {IEEE Internet of Things Journal},
  volume = {3},
  number = {5},
  pages = {637--646},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2016.2579198},
  url = {https://ieeexplore.ieee.org/document/7488250},
  urldate = {2024-09-19},
  abstract = {The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.},
  eventtitle = {{{IEEE Internet}} of {{Things Journal}}},
  keywords = {Bandwidth,Cloud computing,Data privacy,Edge computing,Internet of things,Internet of Things (IoT),Mobile handsets,smart home and city,Smart homes,Time factors}
}

@inproceedings{solomonFunctionalReactiveProgramming2021,
  title = {Functional Reactive Programming and the Web Audio {{API}}},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Solomon, Mike},
  editor = {Joglar-Ongay, Luis and Serra, Xavier and Font, Frederic and Tovstogan, Philip and Stolfi, Ariane and A. Correya, Albin and Ramires, Antonio and Bogdanov, Dmitry and Faraldo, Angel and Favory, Xavier},
  date = {2021-07},
  series = {Wac '21},
  publisher = {UPF},
  location = {Barcelona, Spain},
  issn = {2663-5844},
  abstract = {Functional Reactive Programming (FRP) is a way to model temporal phenomena using events, which carry information corresponding to a precise moment in time, and behaviors, which represent time-varying values. This paper shows how FRP can be used to build reactive audio applications that blend the WebAudio API with other browser-based APIs, such as mouse events and MIDI events. It will start by presenting a brief history of FRP as well as definitions of the Event and Behavior types. It will then discuss the principal challenges of applying the behavior pattern to WebAudio and how these challenges can be solved by using induction on existentially-quantified and linearly-typed Indexed Cofree Comonads. An implementation of this approach is provided via the library purescript-wags.}
}

@article{stapletonAmbiguousDevicesImprovisation2021,
  title = {Ambiguous {{Devices}}: {{Improvisation}}, Agency, Touch and Feedthrough in Distributed Music Performance},
  shorttitle = {Ambiguous {{Devices}}},
  author = {Stapleton, Paul and Davis, Tom},
  date = {2021-04},
  journaltitle = {Organised Sound},
  volume = {26},
  number = {1},
  pages = {52--64},
  publisher = {Cambridge University Press},
  location = {Cambridge, United Kingdom},
  issn = {13557718},
  doi = {10.1017/S1355771821000054},
  url = {https://www.proquest.com/docview/2793915909/abstract/61B1D96F24EA4CA0PQ/1},
  urldate = {2024-09-25},
  abstract = {This article documents the processes behind our distributed musical instrument, Ambiguous Devices. The project is motivated by our mutual desire to explore disruptive forms of networked musical interactions in an attempt to challenge and extend our practices as improvisers and instrument makers. We begin by describing the early design stage of our performance ecosystem, followed by a technical description of how the system functions with examples from our public performances and installations. We then situate our work within a genealogy of human–machine improvisation, while highlighting specific values that continue to motivate our artistic approach. These practical accounts inform our discussion of tactility, proximity, effort, friction and other attributes that have shaped our strategies for designing musical interactions. The positive role of ambiguity is elaborated in relation to distributed agency. Finally, we employ the concept of ‘feedthrough’ as a way of understanding the co-constitutive behaviour of communication networks, assemblages and performers.},
  langid = {english},
  pagetotal = {52-64},
  keywords = {Acoustics,Brainstorming,Communication,Communication networks,Design,Microphones,Musical instruments,Musical performances,Sensors}
}

@inproceedings{sticklandDesignRealtimeMultiparty2019,
  title = {Design of a Real-Time Multiparty {{DAW}} Collaboration Application Using Web {{MIDI}} and {{WebRTC}} Apis},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Stickland, Scott and Athauda, Rukshan and Scott, Nathan},
  editor = {Xambó, Anna and Martín, Sara R. and Roma, Gerard},
  date = {2019-12},
  series = {Wac '19},
  pages = {59--64},
  publisher = {NTNU},
  location = {Trondheim, Norway},
  issn = {2663-5844},
  abstract = {Collaborative music production in online environments has seen a renewed focus as developers of Digital Audio Workstation (DAW) software include features that cater to limited synchronous participation and multiparty asynchronous collaboration. A significant restriction of these collaboration platforms is the inability for multiple collaborators to effectively communicate and seamlessly work on a high-fidelity audio project in real-time. This paper outlines the design of a browser-based application that enables real-time collaboration between multiple remote instantiations of an established, mainstream and fully-featured DAW platform over the Internet. The proposed application provides access to, and modification and creation of, high-fidelity audio assets, real-time videoconferencing and control data streaming for communication and synchronised DAW operations through Web Real-Time Communication (WebRTC) and Web MIDI Application Programming Interfaces (APIs). The paper reports on a proof-of-concept implementation and results, including several areas for further research and development.}
}

@book{stieglerTechnicsTime1998,
  title = {Technics and Time},
  author = {Stiegler, Bernard},
  date = {1998},
  series = {Meridian},
  publisher = {Stanford University Press},
  abstract = {1. The fault of Epimetheus -- 2. Disorientation / translated by Stephen Barker -- 3. Cinematic time and the question of malaise.},
  isbn = {978-0-8047-3040-2},
  keywords = {Technology -- Philosophy}
}

@inproceedings{taylorHistoryAudienceSpeaker2017,
  title = {A {{History}} of the {{Audience}} as a {{Speaker Array}}},
  author = {Taylor, Benjamin},
  date = {2017},
  pages = {481--486},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176324},
  url = {https://nime.org/proc/btaylor2017/index.html},
  urldate = {2024-09-19},
  eventtitle = {Proceedings of the {{International Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  langid = {american}
}

@inproceedings{thalmannMobileAudioOntology2016a,
  title = {The {{Mobile Audio Ontology}}: {{Experiencing Dynamic Music Objects}} on {{Mobile Devices}}},
  shorttitle = {The {{Mobile Audio Ontology}}},
  booktitle = {2016 {{IEEE Tenth International Conference}} on {{Semantic Computing}} ({{ICSC}})},
  author = {Thalmann, Florian and Carrillo, Alfonso Perez and Fazekas, György and Wiggins, Geraint A. and Sandler, Mark},
  date = {2016-02},
  pages = {47--54},
  doi = {10.1109/ICSC.2016.61},
  url = {https://ieeexplore.ieee.org/document/7439304},
  urldate = {2024-09-19},
  abstract = {This paper is about the Mobile Audio Ontology, a semantic audio framework for the design of novel music consumption experiences on mobile devices. The framework is based on the concept of the Dynamic Music Object which is an amalgamation of audio files, structural and analytical information extracted from the audio, and information about how it should be rendered in realtime. The Mobile Audio Ontology allows producers and distributors to specify a great variety of ways of playing back music in controlled indeterministic as well as adaptive and interactive ways. Users can map mobile sensor data, user interface controls, or autonomous control units hidden from the listener to any musical parameter exposed in the definition of a Dynamic Music Object. These mappings can also be made dependent on semantic and analytical information extracted from the audio.},
  eventtitle = {2016 {{IEEE Tenth International Conference}} on {{Semantic Computing}} ({{ICSC}})},
  keywords = {abstract data types,Data mining,digital music players,Feature extraction,mappings,mobile applications,Mobile communication,Mobile handsets,Music,music information retrieval,music representation,Ontologies,Semantic Audio,Semantic Web,Semantics}
}

@book{tomlinsonMillionYearsMusic2015,
  title = {A Million Years of Music : The Emergence of Human Modernity},
  shorttitle = {A Million Years of Music},
  author = {Tomlinson, Gary},
  date = {2015},
  series = {Books at {{JSTOR All Purchased}}},
  publisher = {Zone Books},
  abstract = {Summary: A new narrative for the emergence of human music, drawing from archaeology, cognitive science, linguistics, and evolutionary theory.},
  isbn = {978-1-935408-66-6},
  keywords = {Criticism interpretation etc,Electronic books,History and criticism,Music,Music -- History and criticism,Musicology}
}

@online{turchetBiography,
  title = {About: {{Biography}}},
  author = {Turchet, Luca},
  url = {http://www.lucaturchet.it/en/biografia-en.html},
  urldate = {2024-09-27},
  organization = {Luca Turchet: Sound Designer, Musician, Composer \& Writer}
}

@inproceedings{turchetExamplesUseCases2017,
  title = {Examples of Use Cases with {{Smart Instruments}}},
  booktitle = {Proceedings of the 12th {{International Audio Mostly Conference}} on {{Augmented}} and {{Participatory Sound}} and {{Music Experiences}}},
  author = {Turchet, Luca and Benincaso, Michele and Fischione, Carlo},
  date = {2017-08-23},
  series = {{{AM}} '17},
  pages = {1--5},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3123514.3123553},
  url = {https://dl.acm.org/doi/10.1145/3123514.3123553},
  urldate = {2024-09-20},
  abstract = {This paper presents some of the possibilities for interaction between performers, audiences, and their smart devices, offered by the novel family of musical instruments, the Smart Instruments. For this purpose, some implemented use cases are described, which involved a preliminary prototype of MIND Music Labs' Sensus Smart Guitar, the first exemplar of Smart Instrument. Sensus consists of a guitar augmented with sensors, actuators, onboard processing, and wireless communication. Some of the novel interactions enabled by Sensus technology are presented, which are based on connectivity of the instrument to smart devices, virtual reality headsets, and the cloud.},
  isbn = {978-1-4503-5373-1}
}

@article{turchetInternetAudioThings2020,
  title = {The {{Internet}} of {{Audio Things}}: {{State}} of the {{Art}}, {{Vision}}, and {{Challenges}}},
  shorttitle = {The {{Internet}} of {{Audio Things}}},
  author = {Turchet, Luca and Fazekas, György and Lagrange, Mathieu and Ghadikolaei, Hossein S. and Fischione, Carlo},
  date = {2020-10},
  journaltitle = {IEEE Internet of Things Journal},
  volume = {7},
  number = {10},
  pages = {10233--10249},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2020.2997047},
  url = {https://ieeexplore.ieee.org/document/9099251},
  urldate = {2024-09-19},
  abstract = {The Internet of Audio Things (IoAuT) is an emerging research field positioned at the intersection of the Internet of Things, sound and music computing, artificial intelligence, and human-computer interaction. The IoAuT refers to the networks of computing devices embedded in physical objects (Audio Things) dedicated to the production, reception, analysis, and understanding of audio in distributed environments. Audio Things, such as nodes of wireless acoustic sensor networks, are connected by an infrastructure that enables multidirectional communication, both locally and remotely. In this article, we first review the state of the art of this field, then we present a vision for the IoAuT and its motivations. In the proposed vision, the IoAuT enables the connection of digital and physical domains by means of appropriate information and communication technologies, fostering novel applications and services based on auditory information. The ecosystems associated with the IoAuT include interoperable devices and services that connect humans and machines to support human-human and human-machines interactions. We discuss the challenges and implications of this field, which lead to future research directions on the topics of privacy, security, design of Audio Things, and methods for the analysis and representation of audio-related information.},
  eventtitle = {{{IEEE Internet}} of {{Things Journal}}},
  keywords = {Auditory scene analysis,ecoacoustics,Ecosystems,Internet of Audio Things (IoAuT),Internet of Sounds,Internet of Things,Machine learning,Music,Production,Sensors,smart city}
}

@article{turchetInternetMusicalThings2018,
  title = {Internet of {{Musical Things}}: {{Vision}} and {{Challenges}}},
  shorttitle = {Internet of {{Musical Things}}},
  author = {Turchet, Luca and Fischione, Carlo and Essl, Georg and Keller, DamiáN and Barthet, Mathieu},
  date = {2018},
  journaltitle = {IEEE Access},
  volume = {6},
  pages = {61994--62017},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2872625},
  url = {https://ieeexplore.ieee.org/document/8476543},
  urldate = {2024-09-19},
  abstract = {The Internet of Musical Things (IoMusT) is an emerging research field positioned at the intersection of Internet of Things, new interfaces for musical expression, ubiquitous music, human-computer interaction, artificial intelligence, and participatory art. From a computer science perspective, IoMusT refers to the networks of computing devices embedded in physical objects (musical things) dedicated to the production and/or reception of musical content. Musical things, such as smart musical instruments or wearables, are connected by an infrastructure that enables multidirectional communication, both locally and remotely. We present a vision in which the IoMusT enables the connection of digital and physical domains by means of appropriate information and communication technologies, fostering novel musical applications and services. The ecosystems associated with the IoMusT include interoperable devices and services that connect musicians and audiences to support musician-musician, audience-musicians, and audience-audience interactions. In this paper, we first propose a vision for the IoMusT and its motivations. We then discuss five scenarios illustrating how the IoMusT could support: 1) augmented and immersive concert experiences; 2) audience participation; 3) remote rehearsals; 4) music e-learning; and 5) smart studio production. We identify key capabilities missing from today's systems and discuss the research needed to develop these capabilities across a set of interdisciplinary challenges. These encompass network communication (e.g., ultra-low latency and security), music information research (e.g., artificial intelligence for real-time audio content description and multimodal sensing), music interaction (e.g., distributed performance and music e-learning), as well as legal and responsible innovation aspects to ensure that future IoMusT services are socially desirable and undertaken in the public interest.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Ecosystems,Instruments,Internet of Things,mobile music,Music,networked music performance,participatory art,Real-time systems,Sensors,sound and music computing,ubiquitous music}
}

@article{turchetInternetSoundsConvergent2023,
  title = {The {{Internet}} of {{Sounds}}: Convergent Trends, Insights and Future Directions},
  shorttitle = {The {{Internet}} of {{Sounds}}},
  author = {Turchet, Luca and Lagrange, Mathieu and Rottondi, Cristina and Fazekas, George and Peters, Nils and Østergaard, Jan and Font, Frederic and Backström, Tom and Fischione, Carlo},
  date = {2023},
  publisher = {{Institute of Electrical and Electronics Engineers IEEE}},
  issn = {2327-4662},
  doi = {10.1109/jiot.2023.3253602},
  url = {http://hdl.handle.net/10230/57149},
  urldate = {2024-09-19},
  abstract = {Current sound-based practices and systems developed in both academia and industry point to convergent research trends that bring together the field of Sound and Music Computing with that of the Internet of Things. This paper proposes a vision for the emerging field of the Internet of Sounds (IoS), which stems from such disciplines. The IoS relates to the network of Sound Things, i.e., devices capable of sensing, acquiring, processing, actuating, and exchanging data serving the purpose of communicating sound-related information. In the IoS paradigm, which merges under a unique umbrella the emerging fields of the Internet of Musical Things and the Internet of Audio Things, heterogeneous devices dedicated to musical and non-musical tasks can interact and cooperate with one another and with other things connected to the Internet to facilitate sound-based services and applications that are globally available to the users. We survey the state of the art in this space, discuss the technological and non-technological challenges ahead of us and propose a comprehensive research agenda for the field.},
  langid = {english},
  keywords = {Biotic communities,Hardware,Internet of things,Marketing research,Music,Software}
}

@article{turchetLatencyReliabilityAnalysis2024,
  title = {Latency and {{Reliability Analysis}} of a {{5G-Enabled Internet}} of {{Musical Things System}}},
  author = {Turchet, Luca and Casari, Paolo},
  date = {2024-01},
  journaltitle = {IEEE Internet of Things Journal},
  volume = {11},
  number = {1},
  pages = {1228--1240},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2023.3288818},
  url = {https://ieeexplore.ieee.org/document/10162178},
  urldate = {2024-09-19},
  abstract = {The availability of high-performance embedded audio systems, along with high-bandwidth and low-latency connectivity options provided by 5G networks, is enabling the Internet of Musical Things (IoMusT) paradigm. A central component of this paradigm is represented by networked music performances (NMPs), where geographically displaced musicians play together over the network in real time. However, to date, IoMusT deployments over 5G networks remain scarce, and very limited statistical results are available on the actual latency and reliability of 5G networks for IoMusT and NMP scenarios. In this article, we present a private 5G IoMusT deployment and analyze its performance when supporting NMPs. Our IoMusT system is composed of up to four nodes and includes different background traffic conditions. We focused on the assessment of the sole wireless link, as the measurements can be easily transferred to a realistic NMP architecture involving a wide area network (WAN) by compounding them with those of the WAN. Our results show that latency increases with the number of nodes and with the presence of background traffic, whereas the reliability did not vary with the complexity of the conditions. For all tested scenarios, the average measured latency was below 24 ms (including a jitter buffer of 10.66 ms), whereas packet losses occurred with a probability of less than 0.01. However, irregular spikes were found for all latency and reliability metrics, which can significantly reduce the quality of service perceived by the users of NMP applications. Finally, packet loss and latency resulted to be uncorrelated, which suggests that they have different root causes.},
  eventtitle = {{{IEEE Internet}} of {{Things Journal}}},
  keywords = {5G mobile communication,5G networks,Internet of Musical Things (IoMusT),Internet of Things,low-latency wireless communications,Music,networked music performance (NMP) systems,Performance evaluation,Quality of service,Reliability,Wide area networks}
}

@inproceedings{turchetMakingPhysicalControl2020,
  title = {On Making Physical the Control of Audio Plugins: The Case of the Retrologue Hardware Synthesizer},
  shorttitle = {On Making Physical the Control of Audio Plugins},
  booktitle = {Proceedings of the 15th {{International Audio Mostly Conference}}},
  author = {Turchet, Luca and Willis, Samuel Joel and Andersson, Gustav and Gianelli, Alberto and Benincaso, Michele},
  date = {2020-09-16},
  series = {{{AM}} '20},
  pages = {146--151},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3411109.3411114},
  url = {https://dl.acm.org/doi/10.1145/3411109.3411114},
  urldate = {2024-09-20},
  abstract = {This paper reports the development of a prototype of smart musical instrument that uses a virtual analog audio plugin in conjunction with a dedicated tangible interface and a platform for embedded audio. The adopted design approach started from an analog synthesizer, passed from its digital emulation, and returned to the analog domain via the real-time, physical control of the digital synthesizer. The prototype can be considered as an instance of a class of musical devices that allow one to give physical form to the control of virtual analog software. We present an analysis of online sources that were retrieved following the release of the prototype at an international music trade show. Overall, results preliminary validate the concept underlying the development of the prototype and reveal its potential for both digital musical instruments development and use. Benefits of the proposed class of musical devices include a higher degree of control intimacy of a plugin compared to its use with conventional interfaces such as mice and screens of desktop computers, as well as the use of audio plugins in ubiquitous musical activities.},
  isbn = {978-1-4503-7563-4}
}

@article{turchetRealTimeHitClassification2018,
  title = {Real-{{Time Hit Classification}} in a {{Smart Cajón}}},
  author = {Turchet, Luca and McPherson, Andrew and Barthet, Mathieu},
  date = {2018-07-10},
  journaltitle = {Frontiers in ICT},
  shortjournal = {Front. ICT},
  volume = {5},
  publisher = {Frontiers},
  issn = {2297-198X},
  doi = {10.3389/fict.2018.00016},
  url = {https://www.frontiersin.org/journals/ict/articles/10.3389/fict.2018.00016/full},
  urldate = {2024-09-20},
  abstract = {Smart musical instruments are a class of IoT devices for music making, which encompass embedded intelligence as well as wireless connectivity. In previous work, we established design requirements for a novel smart musical instrument, a smart cajón, following a user-centered approach. This paper describes the implementation and technical evaluation of the designed component of the smart cajón related to hit classification and repurposing. A conventional acoustic cajón was enhanced with sensors to classify position of the hit and the gesture that produced it. The instrument was equipped with five piezo pickups attached to the internal panels and a condenser microphone located inside. The developed sound engine leveraged digital signal processing, sensor fusion, and machine learning techniques to classify the position, dynamics, and timbre of each hit. The techniques were devised and implemented to achieve low latency between action and the electronically-generated sounds, as well as keep computational efficiency high. The system was tuned to classify two main cajón playing techniques at different locations and we conducted evaluations using over 2,000 hits performed by two professional players. We first assessed the classification performance when training and testing data related to recordings from the same player. In this configuration, classification accuracies of 100\% were obtained for hit detection and location. Accuracies of over 90\% were obtained when classifying timbres produced by the two playing techniques. We then assessed the classifier in a cross-player configuration (training and testing were performed using recordings from different players). Results indicated that while hit location scales relatively well across different players, gesture identification requires that the involved classifiers are trained specifically for each musician.},
  langid = {english},
  keywords = {cajón,Internet of Musical Things,machine learning,music information retrieval,Sensor Fusion,smart musical instruments}
}

@article{turchetRelationFieldsNetworked2023,
  title = {On the Relation between the Fields of {{Networked Music Performances}}, {{Ubiquitous Music}}, and {{Internet}} of {{Musical Things}}},
  author = {Turchet, Luca and Rottondi, Cristina},
  date = {2023},
  journaltitle = {Personal and ubiquitous computing},
  volume = {27},
  number = {5},
  pages = {1783--1792},
  publisher = {Springer London},
  location = {London},
  issn = {1617-4909},
  doi = {10.1007/s00779-022-01691-z},
  abstract = {In the past two decades, we have witnessed the diffusion of an increasing number of technologies, products, and applications at the intersection of music and networking. As a result of the growing attention devoted by academy and industry to this area, three main research fields have emerged and progressively consolidated: the Networked Music Performances, Ubiquitous Music, and the Internet of Musical Things. Based on the review of the most relevant works in these fields, this paper attempts to delineate their differences and commonalities. The aim of this inquiry is helping avoid confusion between such fields and achieve a correct use of the terminology. A trend towards the convergence between such fields has already been identified, and it is plausible to expect that in the future their evolution will lead to a progressive blurring of the boundaries identified today.},
  langid = {english},
  keywords = {Computer networks,Computer science,Experiments,Internet,Mobile computing,Research,Transistors,Ubiquitous computing}
}

@article{turchetSemanticWebMusical2023,
  title = {Semantic {{Web}} of {{Musical Things}}: {{Achieving}} Interoperability in the {{Internet}} of {{Musical Things}}},
  shorttitle = {Semantic {{Web}} of {{Musical Things}}},
  author = {Turchet, Luca and Antoniazzi, Francesco},
  date = {2023},
  journaltitle = {Web semantics},
  volume = {75},
  pages = {100758-},
  publisher = {Elsevier B.V},
  issn = {1570-8268},
  doi = {10.1016/j.websem.2022.100758},
  abstract = {The Internet of Musical Things (IoMusT) refers to the extension of the Internet of Things paradigm to the musical domain. Interoperability represents a central issue within this domain, where heterogeneous Musical Things serving radically different purposes are envisioned to communicate between each other. Automatic discovery of resources is also a desirable feature in IoMusT ecosystems. However, the existing musical protocols are not adequate to support discoverability and interoperability across the wide heterogeneity of Musical Things, as they are typically not flexible, lack high resolution, are not equipped with inference mechanisms that could exploit on board the information on the whole application environment. Besides, they hardly ever support easy integration with the Web. In addition, IoMusT applications are often characterized by strict requirements in terms of latency of the exchanged messages. Semantic Web of Things technologies have the potential to overcome the limitations of existing musical protocols by enabling discoverability and interoperability across heterogeneous Musical Things. In this paper we propose the Musical Semantic Event Processing Architecture (MUSEPA), a semantically-based architecture designed to meet the IoMusT requirements of low-latency communication, discoverability, interoperability, and automatic inference. The architecture is based on the CoAP protocol, a semantic publish/subscribe broker, and the adoption of shared ontologies for describing Musical Things and their interactions. The code implementing MUSEPA can be accessed at: https://github.com/CIMIL/MUSEPA/.},
  langid = {english}
}

@inproceedings{turchetSmartMandolinAutobiographical2018,
  title = {Smart {{Mandolin}}: Autobiographical Design, Implementation, Use Cases, and Lessons Learned},
  shorttitle = {Smart {{Mandolin}}},
  booktitle = {Proceedings of the {{Audio Mostly}} 2018 on {{Sound}} in {{Immersion}} and {{Emotion}}},
  author = {Turchet, Luca},
  date = {2018-09-12},
  series = {{{AM}} '18},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3243274.3243280},
  url = {https://dl.acm.org/doi/10.1145/3243274.3243280},
  urldate = {2024-09-20},
  abstract = {This paper presents the Smart Mandolin, an exemplar of the family of the so-called smart instruments. Developed according to the paradigms of autobiographical design, it consists of a conventional acoustic mandolin enhanced with different types of sensors, a microphone, a loudspeaker, wireless connectivity to both local networks and the Internet, and a low-latency audio processing board. Various implemented use cases are presented, which leverage the smart qualities of the instrument. These include the programming of the instrument via applications for smartphones and desktop computer, as well as the wireless control of devices enabling multimodal performances such as screen projecting visuals, smartphones, and tactile devices used by the audience. The paper concludes with an evaluation conducted by the author himself after extensive use, which pinpointed pros and cons of the instrument and provided a comparison with the Hyper-Mandolin, an instance of augmented instruments previously developed by the author.},
  isbn = {978-1-4503-6609-0}
}

@article{turchetSmartMusicalInstruments2022,
  title = {The {{Smart Musical Instruments Ontology}}},
  author = {Turchet, Luca and Bouquet, Paolo and Molinari, Andrea and Fazekas, György},
  date = {2022-04-01},
  journaltitle = {Journal of Web Semantics},
  shortjournal = {Journal of Web Semantics},
  volume = {72},
  pages = {100687},
  issn = {1570-8268},
  doi = {10.1016/j.websem.2021.100687},
  url = {https://www.sciencedirect.com/science/article/pii/S1570826821000573},
  urldate = {2024-09-19},
  abstract = {The Smart Musical Instruments (SMIs) are an emerging category of musical instruments that belongs to the wider class of Musical Things within the Internet of Musical Things paradigm. SMIs encompass sensors, actuators, embedded intelligence, and wireless connectivity to local networks and to the Internet. Interoperability represents a key issue within this domain, where heterogeneous SMIs are envisioned to exchange information between each other and a plethora of Musical Things. This paper proposes an ontology for the representation of the knowledge related to SMIs, with the aim of facilitating interoperability between SMIs as well as with other Musical Things interacting with them. There was no previous comprehensive data model for the SMIs domain, however the new ontology relates to existing ontologies, including the SOSA Ontology for the representation of sensors and actuators, the Audio Effects Ontology dealing with the description of digital audio effects, and the IoMusT Ontology for the representation Musical Things and IoMusT ecosystems. This paper documents the design of the ontology and its evaluation with respect to specific requirements gathered from an extensive literature review, which was based on scenarios involving SMIs stakeholders, such as performers and studio producers. The SMI Ontology can be accessed at: https://w3id.org/smi\#.},
  keywords = {Internet of Musical Things,Semantic audio,Smart Musical Instruments}
}

@article{vansteenBriefIntroductionDistributed2016,
  title = {A Brief Introduction to Distributed Systems},
  author = {family=Steen, given=Maarten, prefix=van, useprefix=true and Tanenbaum, Andrew S.},
  date = {2016},
  journaltitle = {Computing},
  volume = {98},
  number = {10},
  pages = {967--1009},
  publisher = {Springer Vienna},
  location = {Vienna},
  issn = {0010-485X},
  doi = {10.1007/s00607-016-0508-7},
  abstract = {Distributed systems are by now commonplace, yet remain an often difficult area of research. This is partly explained by the many facets of such systems and the inherent difficulty to isolate these facets from each other. In this paper we provide a brief overview of distributed systems: what they are, their general design goals, and some of the most common types.},
  langid = {english},
  keywords = {Artificial intelligence,Communication,Computer networks,Computer science,Computer software,Computers,Engineering design,Software engineering}
}

@inproceedings{vogtenhuberResponsiveSpaceLiveness2019,
  title = {Responsive {{Space}}. {{Liveness}} through Spatial Distribution of Sound and Image.},
  booktitle = {Proceedings of the International Web Audio Conference},
  author = {Vogtenhuber, Raimund},
  editor = {Xambó, Anna and Martín, Sara R. and Roma, Gerard},
  date = {2019-12},
  series = {Wac '19},
  pages = {103--107},
  publisher = {NTNU},
  location = {Trondheim, Norway},
  issn = {2663-5844},
  abstract = {In this project a performance framework called 'Responsive Space' has been created, which allows a flexible way of working with distributed sound and image projections. The system consists of a multichannel speaker-system, up to three video projections, a local Wifi network with a connected webserver and mobile devices. The audience is invited to log into the local network with their mobile devices. In the browser of the mobile devices sound and image is generated or streamed.This offers the possibility of a great diversity in the spatial distribution of sound and image. As experienced in previous projects with live audio-streaming and mobile devices the spatial gesture of music as a musical parameter comes to the fore. In the performances the audio is streamed with an icecast-server and visual output is generated with javascript on the client’s browser. The imperfection of time synchronization leads to very interesting effects. The awareness of space and the interaction of the audience with each other leads to the emergence of a social space in the concert.In addition to the dimension of space this setup also examines the relationship between different media layers. This is central for the question how to achieve 'liveness' - which in my opinion can also be achieved with this spatial arrangement and distribution of sound and image.}
}

@article{wangAudienceParticipationTechniques,
  title = {Audience {{Participation Techniques Based}} on {{Social Mobile Computing}}},
  author = {Wang, Ge},
  url = {https://www.academia.edu/1982198/Audience_Participation_Techniques_Based_on_Social_Mobile_Computing},
  urldate = {2024-09-28},
  abstract = {Audience Participation Techniques Based on Social Mobile Computing}
}

@incollection{wangSurveyEdgeIntelligence,
  title = {A {{Survey}} on~{{Edge Intelligence}} for~{{Music Composition}}: {{Principles}}, {{Applications}}, and~{{Privacy Implications}}},
  shorttitle = {A {{Survey}} on~{{Edge Intelligence}} for~{{Music Composition}}},
  booktitle = {Tools for {{Design}}, {{Implementation}} and {{Verification}} of {{Emerging Information Technologies}}},
  author = {Wang, Qinyuan and Qu, Youyang and Nan, Siyu and Jiang, Wantong and Gu, Bruce and Gu, Shujun},
  namea = {Huang, Xinyi and Liu, Jianghua and Xu, Lei},
  nameatype = {collaborator},
  series = {Lecture {{Notes}} of the {{Institute}} for {{Computer Sciences}}, {{Social Informatics}} and {{Telecommunications Engineering}}},
  pages = {41--74},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  issn = {1867-8211},
  doi = {10.1007/978-3-031-51399-2_3},
  abstract = {The field of music composition has seen significant advancements with the introduction of artificial intelligence (AI) techniques. However, traditional cloud-based approaches suffer from limitations such as latency and network dependency. This survey paper explores the emerging concept of edge intelligence and its application in music composition. Edge intelligence leverages local computational resources to enable real-time and on-device music generation, enhancing the creative process and expanding accessibility. By examining various aspects of music composition, including melody creation, harmonization, rhythm generation, arrangement and orchestration, and lyric writing, this paper showcases the potential benefits of incorporating edge intelligence. It also discusses the challenges and limitations associated with this paradigm, such as limited computational resources and privacy concerns. Through a review of existing AI-based music composition tools and platforms, examples of edge intelligence in action are highlighted. The survey paper concludes by emphasizing the transformative potential of edge intelligence in revolutionizing the field of music composition and identifies future research opportunities to further advance this promising domain.},
  isbn = {978-3-031-51398-5},
  langid = {english},
  keywords = {Artificial intelligence,Composition (Music)}
}

@book{warkCapitalDeadThis2021,
  title = {Capital {{Is Dead}}: {{Is This Something Worse}}?},
  shorttitle = {Capital {{Is Dead}}},
  author = {Wark, McKenzie},
  date = {2021-02-09},
  edition = {Reprint edition},
  publisher = {Verso},
  location = {London ; New York},
  abstract = {It's not capitalism, it's not neoliberalism - what if it's something worse?In this radical and visionary new book, McKenzie Wark argues that information has empowered a new kind of ruling class. Through the ownership and control of information, this emergent class dominates not only labour but capital as traditionally understood as well. And it’s not just tech companies like Amazon and Google. Even Walmart and Nike can now dominate the entire production chain through the ownership of not much more than brands, patents, copyrights, and logistical systems.While techno-utopian apologists still celebrate these innovations as an improvement on capitalism, for workers—and the planet—it’s worse. The new ruling class uses the powers of information to route around any obstacle labor and social movements put up. So how do we find a way out?~Capital Is Dead~offers not only the theoretical tools to analyze this new world, but ways to change it. Drawing on the writings of a surprising range of classic and contemporary theorists, Wark offers an illuminating overview of the contemporary condition and the emerging class forces that control—and contest—it.},
  isbn = {978-1-78873-533-9},
  langid = {english},
  pagetotal = {208}
}

@article{weinbergInterconnectedMusicalNetworks2005,
  title = {Interconnected {{Musical Networks}}: {{Toward}} a {{Theoretical Framework}}},
  shorttitle = {Interconnected {{Musical Networks}}},
  author = {Weinberg, Gil},
  date = {2005},
  journaltitle = {Computer music journal},
  volume = {29},
  number = {2},
  pages = {23--39},
  publisher = {MIT Press},
  location = {238 Main St., Suite 500, Cambridge, MA 02142-1046, USA},
  issn = {0148-9267},
  doi = {10.1162/0148926054094350},
  abstract = {This article attempts to define and classify the aesthetic and technical principles of interconnected musical networks. It presents an historical overview of technological innovations that were instrumental for the development of the field and discusses a number of paradigmatic musical networks that are based on these technologies. A classification of online and local-area musical networks then leads to an attempt to define a taxonomical and theoretical framework for musical interconnectivity, addressing goals and motivations, social organizations and perspectives, network architectures and topologies, and musical content and control. The article concludes with a number of design suggestions for the development of effective interconnected musical networks.},
  langid = {english},
  keywords = {Composition,Composition (Music),Computer music,Computer networks,Electronic music,Music,Musical instruments}
}

@article{weiserComputer21stCentury2002,
  title = {The Computer for the 21st {{Century}}},
  author = {Weiser, M.},
  date = {2002},
  journaltitle = {IEEE pervasive computing},
  volume = {1},
  number = {1},
  pages = {19--25},
  publisher = {IEEE},
  location = {New York},
  issn = {1536-1268},
  doi = {10.1109/MPRV.2002.993141},
  abstract = {[This article is unavailable due to copyright restrictions.] Specialized elements of hardware and software, connected by wires, radio waves and infrared, will soon be so ubiquitous that no-one will notice their presence.},
  langid = {english},
  keywords = {Computer programs,Computer software,Hardware,Information technology,Microcomputers,Portable computers,Radio waves,Software,Ubiquitous computing,Virtual reality,Wire}
}

@article{wilsonAestheticTechnicalStrategies2023,
  title = {Aesthetic and Technical Strategies for Networked Music Performance},
  author = {Wilson, Rebekah},
  date = {2023},
  journaltitle = {AI \& society},
  volume = {38},
  number = {5},
  pages = {1871--1884},
  publisher = {Springer London},
  location = {London},
  issn = {0951-5666},
  doi = {10.1007/s00146-020-01099-4},
  abstract = {Networked music is no longer a future genre: the global quarantine event of 2020 launched the concept of performing together over the Internet into the mainstream. While the demand for performing at a distance may be a new imperative, musicians find themselves faced with technological and performative processes that do not appear to be suitable for performing music together online, due particularly to network latency which disrupts the ability for musicians to synchronize. The research presented in this paper investigates and challenges the reasons why networked music is not readily embraced by musicians and describes how that might change, by way of interviews with practitioners and an in-depth review of the technical constraints. Limitations that might cause frustration are in fact shown to have creative strategies that give rise to aesthetic approaches, distinct to the platform. By exploiting the constraints, in tandem with developing technology designed specifically for remote performance, aesthetic implications arise that not only accommodate the inconveniences of latency and acoustic feedback but can help us adapt and transform how we engage in real-time online, towards a future where we can imagine performing together over even more dramatic distances such as high-latency, low-bandwidth locations outside of urban areas—or even over galactic distances.},
  langid = {english},
  keywords = {Artificial intelligence,Cities and towns,Computer science,Control,Engineering economy,Frustration,Logistics,Marketing,Mechatronics,Metropolitan areas,Music,Organization,Performing arts,Robotics}
}

@article{wyseViabilityWebBrowser2013,
  title = {The {{Viability}} of the {{Web Browser}} as a {{Computer Music Platform}}},
  author = {Wyse, Lonce and Subramanian, Srikumar},
  date = {2013},
  journaltitle = {Computer music journal},
  volume = {37},
  number = {4},
  pages = {10--23},
  publisher = {MIT Press},
  location = {55 Hayward St., Cambridge, MA 02142-1315, USA},
  issn = {0148-9267},
  doi = {10.1162/COMJ_a_00213},
  abstract = {The computer music community has historically pushed the boundaries of technologies for music-making, using and developing cutting-edge computing, communication, and interfaces in a wide variety of creative practices to meet exacting standards of quality. Several separate systems and protocols have been developed to serve this community, such as Max/MSP and Pd for synthesis and teaching, JackTrip for networked audio, MIDI/OSC for communication, as well as Max/MSP and TouchOSC for interface design, to name a few. With the still-nascent Web Audio API standard and related technologies, we are now, more than ever, seeing an increase in these capabilities and their integration in a single ubiquitous platform: the Web browser. In this article, we examine the suitability of the Web browser as a computer music platform in critical aspects of audio synthesis, timing, I/O, and communication. We focus on the new Web Audio API and situate it in the context of associated technologies to understand how well they together can be expected to meet the musical, computational, and development needs of the computer music community. We identify timing and extensibility as two key areas that still need work in order to meet those needs.},
  langid = {english},
  keywords = {Browsers (Computer programs),Communication,MP3 (Audio coding standard)}
}
